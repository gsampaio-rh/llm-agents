{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools LangChain Agent\n",
    "\n",
    "In this notebook, we explore how to set up a LangChain agent that can use external tools to enhance its capabilities. This type of agent is called a \"Tool Calling Agent.\" It can perform tasks such as fetching data from the web, querying APIs, or accessing specialized knowledge bases. \n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **What is a Tool Calling Agent?**\n",
    "2. **Setting Up the Environment**\n",
    "3. **Creating and Configuring the Agent**\n",
    "4. **Using External Tools with the Agent**\n",
    "5. **Examples of Tool Usage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a Tool Calling Agent?\n",
    "\n",
    "A **Tool Calling Agent** in LangChain is an AI agent that can call external tools to perform specific tasks. These tools can include APIs, databases, or other services that provide additional data or functionality. By integrating these tools, the agent can go beyond its built-in capabilities and provide more accurate or specialized responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Environment\n",
    "\n",
    "Before we create our agent, we need to install some necessary libraries. These libraries will allow us to create the agent, connect to external tools, and manage data.\n",
    "\n",
    "### Install Necessary Libraries\n",
    "\n",
    "To get started, install the following libraries using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "%pip install langchain langchain_community langchain_core langchainhub langchain-ollama faiss-cpu arxiv wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating and Configuring the Agent\n",
    "\n",
    "In this section, we will create the main components of our agent, including the language model and any required tools\n",
    "\n",
    "### Import Necessary Modules\n",
    "\n",
    "First, we import the necessary modules from the `langchain`, `langchain_ollama`, and `langchain_community` packages. These modules include tools for loading models, fetching data, and handling various tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent, tool\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import  ArxivQueryRun\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Language Model\n",
    "\n",
    "We set up the language model (LLM) that the agent will use to process natural language. Here, we use the `ChatOllama` model, a versatile LLM capable of handling various text-based tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3-groq-tool-use\",\n",
    "    # Optional parameters to fine-tune model behavior\n",
    "    temperature=1,  # Controls randomness in responses\n",
    "    max_new_tokens=512  # Limits the length of the output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up External Tools\n",
    "\n",
    "External tools can be anything from APIs to databases that provide data or perform specific tasks. Here, we set up some tools like `WikipediaAPIWrapper` and `ArxivAPIWrapper` for fetching information from Wikipedia and ArXiv, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Loader and Retriever Tool Setup\n",
    "\n",
    "We use the `WebBasedLoader` to load documents from a specified URL and `FAISS` for vector-based similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x105068850>)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=docs[:30],\n",
    "     embedding=OllamaEmbeddings(model=\"dolphin-llama3\",show_progress=True),\n",
    ")\n",
    "retriever = vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith_search'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool = create_retriever_tool(retriever, \"langsmith_search\", \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\")\n",
    "\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia Tool Setup & ArXiv Tool Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the List of Tools\n",
    "\n",
    "Now that we have defined our tools, we can combine them into our agent. The agent will use these tools to handle specific types of queries, enhancing its capabilities to provide more accurate and relevant information.\n",
    "\n",
    "We define a list of the tools we created, which the agent can call upon as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/Users/gsampaio/redhat/ai/llm-agents/.conda/lib/python3.11/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " Tool(name='langsmith_search', description='Search for information about LangSmith. For any questions about LangSmith, you must use this tool!', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x10f6922a0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x105068850>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x10f692660>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x105068850>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [wiki, arxiv, retriever_tool]\n",
    "\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the Prompt Template\n",
    "\n",
    "The prompt template defines the structure of the questions or instructions given to the agent. We can use an existing prompt template from a library or customize our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can.\n",
      "You can answer directly if the user is greeting you or similar.\n",
      "Otherise, you have access to the following tools:\n",
      "\n",
      "wikipedia - A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query., args: {{'query': {{'title': 'Query', 'description': 'query to look up on wikipedia', 'type': 'string'}}}}\n",
      "arxiv - A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query., args: {{'query': {{'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}}}\n",
      "langsmith_search(query: 'str', *, retriever: 'BaseRetriever' = VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x105068850>), document_prompt: 'BasePromptTemplate' = PromptTemplate(input_variables=['page_content'], template='{{page_content}}'), document_separator: 'str' = '\\n\\n', callbacks: 'Callbacks' = None) -> 'str' - Search for information about LangSmith. For any questions about LangSmith, you must use this tool!, args: {{'query': {{'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}}}\n",
      "\n",
      "The way you use the tools is by specifying a json blob.\n",
      "Specifically, this json should have a `action` key (with the name of the tool to use)\n",
      "and a `action_input` key (with the input to the tool going here).\n",
      "The only values that should be in the \"action\" field are: ['wikipedia', 'arxiv', 'langsmith_search']\n",
      "The $JSON_BLOB should only contain a SINGLE action, \n",
      "do NOT return a list of multiple actions.\n",
      "Here is an example of a valid $JSON_BLOB:\n",
      "```\n",
      "{{\n",
      "    \"action\": $TOOL_NAME,\n",
      "    \"action_input\": $INPUT\n",
      "}}\n",
      "```\n",
      "The $JSON_BLOB must always be enclosed with triple backticks!\n",
      "\n",
      "ALWAYS use the following format:\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: \n",
      "Observation: the result of the action... \n",
      "(this Thought/Action/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Reminder to always use the exact characters `Final Answer` when responding.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools.render import render_text_description_and_args\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# chat_model_with_stop = llm.bind(stop=[\"\\nObservation\"])\n",
    "\n",
    "# Inspiration taken from hub.pull(\"hwchase17/react-json\")\n",
    "system_message = f\"\"\"Answer the following questions as best you can.\n",
    "You can answer directly if the user is greeting you or similar.\n",
    "Otherise, you have access to the following tools:\n",
    "\n",
    "{render_text_description_and_args(tools).replace('{', '{{').replace('}', '}}')}\n",
    "\n",
    "The way you use the tools is by specifying a json blob.\n",
    "Specifically, this json should have a `action` key (with the name of the tool to use)\n",
    "and a `action_input` key (with the input to the tool going here).\n",
    "The only values that should be in the \"action\" field are: {[t.name for t in tools]}\n",
    "The $JSON_BLOB should only contain a SINGLE action, \n",
    "do NOT return a list of multiple actions.\n",
    "Here is an example of a valid $JSON_BLOB:\n",
    "```\n",
    "{{{{\n",
    "    \"action\": $TOOL_NAME,\n",
    "    \"action_input\": $INPUT\n",
    "}}}}\n",
    "```\n",
    "The $JSON_BLOB must always be enclosed with triple backticks!\n",
    "\n",
    "ALWAYS use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: \n",
    "Observation: the result of the action... \n",
    "(this Thought/Action/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Reminder to always use the exact characters `Final Answer` when responding.'\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"user\",\n",
    "            system_message,\n",
    "        ),\n",
    "        # MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(system_message)\n",
    "\n",
    "# Here, we are using prompt template that is suitable for working with the tools we have integrated. You can modify this prompt template as needed to better fit your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating and Using the Tool Calling Agent\n",
    "\n",
    "### Creating the Agent\n",
    "\n",
    "We create the Tool Calling Agent using the `create_tool_calling_agent` function, which integrates the language model, tools, and prompt template.\n",
    "\n",
    "In this setup:\n",
    "\n",
    "- **llm**: The language model used to interpret and generate text.\n",
    "- **tools**: The list of tools that the agent can use.\n",
    "- **prompt**: The prompt template defining the agent's input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the prompt to use - you can modify this!\n",
    "# prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# prompt.messages\n",
    "# prompt.pretty_print()\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Agent\n",
    "\n",
    "Finally, we can use the agent to process a specific input query. The agent will decide which tools to use based on the query and provide a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'LangSmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;200m\u001b[1;3m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Get started with LangSmith | 🦜️🛠️ LangSmith\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith​PythonTypeScriptpip install -U langsmithyarn add langsmith2. Create an API key​To create an API key head to the Settings page. Then click Create API Key.3. Set up your environment​Shellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>4. Log your first trace​We provide multiple ways to log traces to LangSmith. Below, we'll highlight\n",
      "how to use traceable. See more on the Annotate code for tracing page.PythonTypeScriptimport openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{\"role\": \"user\", \"content\": user_input}],        model=\"gpt-3.5-turbo\"    )    return result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input }],        model: \"gpt-3.5-turbo\",    });    return result.choices[0].message.content;});await pipeline(\"Hello, world!\")// Out: Hello there! How can I assist you today?View a sample output trace.Learn more about tracing in the how-to guides.5. Run your first evaluation​Evaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator.PythonTypeScriptfrom langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name = \"Sample Dataset\"dataset = client.create_dataset(dataset_name, description=\"A sample dataset in LangSmith.\")client.create_examples(    inputs=[        {\"postfix\": \"to LangSmith\"},        {\"postfix\": \"to Evaluations in LangSmith\"},    ],    outputs=[        {\"output\": \"Welcome to LangSmith\"},        {\"output\": \"Welcome to Evaluations in LangSmith\"},    ],    dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example):    return {\"score\": run.outputs[\"output\"] == example.outputs[\"output\"]}experiment_results = evaluate(    lambda input: \"Welcome \" + input['postfix'], # Your AI system goes here    data=dataset_name, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=\"sample-experiment\", # The name of the experiment    metadata={      \"version\": \"1.0.0\",      \"revision_id\": \"beta\"    },)import { Client, Run, Example } from \"langsmith\";import { evaluate } from \"langsmith/evaluation\";import { EvaluationResult } from \"langsmith/evaluation\";const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {  description: \"A sample dataset in LangSmith.\",});await client.createExamples({  inputs: [    { postfix: \"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { output: \"Welcome to LangSmith\" },    { output: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: \"exact_match\",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: {      version: \"1.0.0\",      revision_id: \"beta\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?You can leave detailed feedback on GitHub.NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2024 LangChain, Inc.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Nautico Capibaribe'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Clube Náutico Capibaribe\n",
      "Summary: Clube Náutico Capibaribe (Portuguese pronunciation: [ˈklubi ˈnawtʃiku kɐpibɐˈɾibi]), or simply Náutico, is a Brazilian multi-sport club based in Recife, Pernamb\u001b[0m\u001b[32;1m\u001b[1;3mHere are the answers:\n",
      "Langsmith: LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "Nautico Capibaribe: Clube Náutico Capibaribe (Portuguese pronunciation: [ˈklubi ˈnawtʃiku kɐpibɐˈɾibi]), or simply Náutico, is a Brazilian multi-sport club based in Recife, Pernambuco.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about Langsmith and then search on wikipedia for Nautico Capibaribe. At the end write the two answers!',\n",
       " 'output': 'Here are the answers:\\nLangsmith: LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nNautico Capibaribe: Clube Náutico Capibaribe (Portuguese pronunciation: [ˈklubi ˈnawtʃiku kɐpibɐˈɾibi]), or simply Náutico, is a Brazilian multi-sport club based in Recife, Pernambuco.'}"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this example, the agent is queried with \"Tell me about Langsmith and then search on wikipedia for Nautico Capibaribe. It will use the appropriate tools to gather information and provide an answer.\n",
    "agent_executor.invoke({\"input\":\"Tell me about Langsmith and then search on wikipedia for Nautico Capibaribe. At the end write the two answers!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The function calculates the length of the input string.',\n",
       " {'a': {'title': 'A', 'type': 'string'}})"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import things that are needed generically\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "@tool\n",
    "def calculate_length_tool(a: str) -> int:\n",
    "    \"\"\"The function calculates the length of the input string.\"\"\"\n",
    "    return len(a)\n",
    "\n",
    "calculate_length_tool.description, calculate_length_tool.args,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can.\n",
      "You can answer directly if the user is greeting you or similar.\n",
      "Otherise, you have access to the following tools:\n",
      "\n",
      "wikipedia - A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query., args: {{'query': {{'title': 'Query', 'description': 'query to look up on wikipedia', 'type': 'string'}}}}\n",
      "arxiv - A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query., args: {{'query': {{'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}}}\n",
      "langsmith_search(query: 'str', *, retriever: 'BaseRetriever' = VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x105068850>), document_prompt: 'BasePromptTemplate' = PromptTemplate(input_variables=['page_content'], template='{{page_content}}'), document_separator: 'str' = '\\n\\n', callbacks: 'Callbacks' = None) -> 'str' - Search for information about LangSmith. For any questions about LangSmith, you must use this tool!, args: {{'query': {{'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}}}\n",
      "calculate_length_tool(a: str) -> int - The function calculates the length of the input string., args: {{'a': {{'title': 'A', 'type': 'string'}}}}\n",
      "\n",
      "The way you use the tools is by specifying a json blob.\n",
      "Specifically, this json should have a `action` key (with the name of the tool to use)\n",
      "and a `action_input` key (with the input to the tool going here).\n",
      "The only values that should be in the \"action\" field are: ['wikipedia', 'arxiv', 'langsmith_search', 'calculate_length_tool']\n",
      "The $JSON_BLOB should only contain a SINGLE action, \n",
      "do NOT return a list of multiple actions.\n",
      "Here is an example of a valid $JSON_BLOB:\n",
      "```\n",
      "{{\n",
      "    \"action\": $TOOL_NAME,\n",
      "    \"action_input\": $INPUT\n",
      "}}\n",
      "```\n",
      "The $JSON_BLOB must always be enclosed with triple backticks!\n",
      "\n",
      "ALWAYS use the following format:\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action:```\n",
      "$JSON_BLOB\n",
      "```\n",
      "Observation: the result of the action... \n",
      "(this Thought/Action/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Reminder to always use the exact characters `Final Answer` when responding.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add the calculate_length_tool to the list\n",
    "tools = [wiki, arxiv, retriever_tool, calculate_length_tool]\n",
    "\n",
    "system_message = f\"\"\"Answer the following questions as best you can.\n",
    "You can answer directly if the user is greeting you or similar.\n",
    "Otherise, you have access to the following tools:\n",
    "\n",
    "{render_text_description_and_args(tools).replace('{', '{{').replace('}', '}}')}\n",
    "\n",
    "The way you use the tools is by specifying a json blob.\n",
    "Specifically, this json should have a `action` key (with the name of the tool to use)\n",
    "and a `action_input` key (with the input to the tool going here).\n",
    "The only values that should be in the \"action\" field are: {[t.name for t in tools]}\n",
    "The $JSON_BLOB should only contain a SINGLE action, \n",
    "do NOT return a list of multiple actions.\n",
    "Here is an example of a valid $JSON_BLOB:\n",
    "```\n",
    "{{{{\n",
    "    \"action\": $TOOL_NAME,\n",
    "    \"action_input\": $INPUT\n",
    "}}}}\n",
    "```\n",
    "The $JSON_BLOB must always be enclosed with triple backticks!\n",
    "\n",
    "ALWAYS use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action:```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: the result of the action... \n",
    "(this Thought/Action/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Reminder to always use the exact characters `Final Answer` when responding.'\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"user\",\n",
    "            system_message,\n",
    "        ),\n",
    "        # MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(system_message)\n",
    "\n",
    "# tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What can you tell me about Langsmith?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about Langsmith and calculate the length of the text result.',\n",
       " 'output': 'Question: What can you tell me about Langsmith?'}"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\":\"Tell me about Langsmith and calculate the length of the text result.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Using with chat history\n",
    "# from langchain_core.messages import AIMessage, HumanMessage\n",
    "# agent_executor.invoke(\n",
    "#     {\n",
    "#         \"input\": \"what's my name?\",\n",
    "#         \"chat_history\": [\n",
    "#             HumanMessage(content=\"hi! my name is bob\"),\n",
    "#             AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "#         ],\n",
    "#     }\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
