{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Agents with custom tools and Ollama 3.1\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    "1. **Set Up the Environment**: Prepare your development environment with the necessary tools and libraries.\n",
    "2. **Define Custom Tools**: Create custom functions that can be used by the agent to perform specific tasks.\n",
    "3. **Integrate with Ollama Model**: Use the Ollama 3.1 model to generate responses based on user queries and tool descriptions.\n",
    "4. **Build an Agent**: Combine the tools and the model into an intelligent agent that can process and respond to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts\n",
    "\n",
    "### 1. Intelligent Agents\n",
    "\n",
    "An intelligent agent is a system that perceives its environment and takes actions to achieve specific goals. In this tutorial, our agent will process user inputs and use predefined tools to provide responses. This concept draws inspiration from various frameworks, including task-driven autonomous agents as explored in [this article by Yohei Nakajima](https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/).\n",
    "\n",
    "### 2. Custom Tools\n",
    "\n",
    "Custom tools are user-defined functions that perform specific tasks, such as calculations or data manipulation. These tools will be utilized by the agent to process queries.\n",
    "\n",
    "### 3. Language Models\n",
    "\n",
    "Language models, like Ollama 3.1, are AI models trained to understand and generate human-like text. In this tutorial, the model will help the agent interpret user inputs and determine which tool to use.\n",
    "\n",
    "### 3.1. Llama 3.1\n",
    "\n",
    "**Llama 3.1**, released on July 23, 2024, is Meta's most advanced open-source language model to date. It boasts a massive 405 billion parameters and is designed for a wide range of applications, including general knowledge, steerability, and multilingual translation. With enhanced context length support up to 128K and state-of-the-art capabilities, Llama 3.1 aims to democratize AI by providing developers with the tools to create custom agents and innovative applications. More details can be found in the [official announcement](https://ai.meta.com/blog/meta-llama-3-1/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "Before we dive into building our agent, we need to set up the necessary environment. This involves installing required packages and ensuring our Python environment is ready for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: termcolor in ./.conda/lib/python3.11/site-packages (2.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install termcolor for colored terminal outputs\n",
    "%pip install termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from termcolor import colored\n",
    "import operator\n",
    "import json\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the System Prompt and Model Configuration\n",
    "\n",
    "In this section, we define the system prompt and set up the configuration for the Ollama model. The system prompt provides context and instructions for the model, ensuring it understands how to use the available tools and respond to queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompt Template\n",
    "\n",
    "The system prompt is a crucial component in guiding the behavior of the language model. It provides a structured way to present the available tools and define how the model should respond to user queries.\n",
    "\n",
    "*Based on:*\n",
    "[Model Cards & Prompt formats Llama 3.1](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/#user-defined-custom-tool-calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_system_prompt_template = \"\"\"\n",
    "Environment: ipython\\n\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: July 2024\n",
    "\n",
    "You have access to the following functions:\n",
    "\n",
    "{tool_descriptions}\n",
    "Think very carefully before calling functions.\n",
    "If you choose to call a function ONLY reply in the JSON format with no prefix or suffix:\n",
    "\n",
    "\"tool_choice\": \"name_of_the_tool\",\n",
    "\"tool_input\": \"inputs_to_the_tool\"\n",
    "\n",
    "- `tool_choice`: The name of the tool you want to use. It must be a tool from your toolbox \n",
    "                or \"no tool\" if you do not need to use a tool.\n",
    "- `tool_input`: The specific inputs required for the selected tool. \n",
    "                If no tool, just provide a response to the query.\n",
    "\n",
    "Reminder:\n",
    "- If looking for real time information use relevant functions before falling back to brave_search\n",
    "- Function calls MUST follow the specified format, start with <function= and end with </function>\n",
    "- Required parameters MUST be specified\n",
    "- Only call one function at a time\n",
    "- Put the entire function call reply on one line\n",
    "\n",
    "You are a helpful Assistant.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Ollama Model\n",
    "\n",
    "The `setup_ollama_model` function configures the model settings, including the endpoint, model name, system prompt, and other parameters. This setup is essential for initializing the model with the correct configuration, ensuring it can process queries and utilize the tools effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ollama_model(model, system_prompt, temperature=0, stop=None):\n",
    "    \"\"\"\n",
    "    Sets up the Ollama model configuration.\n",
    "\n",
    "    Parameters:\n",
    "    model (str): The name of the model to use.\n",
    "    system_prompt (str): The system prompt to use.\n",
    "    temperature (float): The temperature setting for the model.\n",
    "    stop (str): The stop token for the model.\n",
    "\n",
    "    Returns:\n",
    "    dict: Configuration for the Ollama model.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model_endpoint\": \"http://localhost:11434/api/generate\",\n",
    "        \"model\": model,\n",
    "        \"system_prompt\": system_prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"stop\": stop,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example configuration\n",
    "ollama_config = setup_ollama_model(\n",
    "    model=\"llama3.1\", system_prompt=agent_system_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating custom tools\n",
    "\n",
    "This section introduces custom tools that our agent will use to perform specific tasks. These tools are functions designed to handle particular operations, such as mathematical calculations and string manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Calculator\n",
    "\n",
    "The `basic_calculator` function is designed to perform arithmetic operations on two numbers based on the input provided. It accepts a JSON string containing the numbers and the operation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_calculator(input_str):\n",
    "    \"\"\"\n",
    "    Perform a numeric operation on two numbers based on the input string.\n",
    "\n",
    "    Parameters:\n",
    "    input_str (str): A JSON string representing a dictionary with keys:\n",
    "        - 'num1' (int): The first number.\n",
    "        - 'num2' (int): The second number.\n",
    "        - 'operation' (str): The operation to perform. Supported operations are 'add', 'subtract',\n",
    "                             'multiply', 'divide', 'floor_divide', 'modulus', 'power', 'lt',\n",
    "                             'le', 'eq', 'ne', 'ge', 'gt'.\n",
    "\n",
    "    Returns:\n",
    "    str: The formatted result of the operation.\n",
    "\n",
    "    Raises:\n",
    "    Exception: If an error occurs during the operation (e.g., division by zero).\n",
    "    ValueError: If an unsupported operation is requested or input is invalid.\n",
    "    \"\"\"\n",
    "    # Clean and parse the input string\n",
    "    try:\n",
    "        # Replace single quotes with double quotes\n",
    "        input_str_clean = input_str.replace(\"'\", '\"')\n",
    "        # Remove any extraneous characters such as trailing quotes\n",
    "        input_str_clean = input_str_clean.strip().strip('\"')\n",
    "\n",
    "        input_dict = json.loads(input_str_clean)\n",
    "        num1 = input_dict[\"num1\"]\n",
    "        num2 = input_dict[\"num2\"]\n",
    "        operation = input_dict[\"operation\"]\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        return str(e), \"Invalid input format. Please provide a valid JSON string.\"\n",
    "\n",
    "    # Define the supported operations\n",
    "    operations = {\n",
    "        \"add\": operator.add,\n",
    "        \"subtract\": operator.sub,\n",
    "        \"multiply\": operator.mul,\n",
    "        \"divide\": operator.truediv,\n",
    "        \"floor_divide\": operator.floordiv,\n",
    "        \"modulus\": operator.mod,\n",
    "        \"power\": operator.pow,\n",
    "        \"lt\": operator.lt,\n",
    "        \"le\": operator.le,\n",
    "        \"eq\": operator.eq,\n",
    "        \"ne\": operator.ne,\n",
    "        \"ge\": operator.ge,\n",
    "        \"gt\": operator.gt,\n",
    "    }\n",
    "\n",
    "    # Check if the operation is supported\n",
    "    if operation in operations:\n",
    "        try:\n",
    "            # Perform the operation\n",
    "            result = operations[operation](num1, num2)\n",
    "            result_formatted = (\n",
    "                f\"\\n\\nThe answer is: {result}.\\nCalculated with basic_calculator.\"\n",
    "            )\n",
    "            return result_formatted\n",
    "        except Exception as e:\n",
    "            return str(e), \"\\n\\nError during operation execution.\"\n",
    "    else:\n",
    "        return \"\\n\\nUnsupported operation. Please provide a valid operation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse String\n",
    "\n",
    "The `reverse_string` function takes a string as input and returns its reverse. This simple tool is useful for tasks involving string manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_string(input_string):\n",
    "    \"\"\"\n",
    "    Reverse the given string.\n",
    "\n",
    "    Parameters:\n",
    "    - input_string (str): The string to be reversed.\n",
    "\n",
    "    Returns:\n",
    "    str: The reversed string.\n",
    "    \"\"\"\n",
    "    # Reverse the string using slicing\n",
    "    reversed_string = input_string[::-1]\n",
    "\n",
    "    reversed_string = f\"The reversed string is: {reversed_string}\\n\\nExecuted using the reverse_string function.\"\n",
    "    # print (f\"DEBUG: reversed_string: {reversed_string}\")\n",
    "    return reversed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use these tools within our agent, we register them in a list. This list will be referenced by the agent to determine which tools are available for use.\n",
    "tools = [basic_calculator, reverse_string]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Agent Class\n",
    "\n",
    "This section focuses on constructing the `Agent` class, which integrates the tools and the Ollama model. The class handles user queries, determines the appropriate tools to use, and executes the necessary actions. This structure encapsulates the core functionalities of the agent, making it a reusable and maintainable component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class Definition\n",
    "\n",
    "The `Agent` class is designed to manage the interaction between user queries, the available tools, and the language model. It includes methods for initializing the agent, preparing tool descriptions, generating responses, and executing tools.\n",
    "\n",
    "- The agent is initialized with a list of tools and a configuration dictionary for the model.\n",
    "- This includes setting up the model's endpoint, name, system prompt, temperature, headers, and stop token.\n",
    "- The `prepare_tools` method is called to set up the descriptions of the tools.\n",
    "\n",
    "**Preparing Tools**\n",
    "\n",
    "The `prepare_tools` method processes the tools' docstrings to extract and format descriptions and parameter information. This is crucial for generating a system prompt that informs the model of the available tools.\n",
    "\n",
    "**Generating Model Responses**\n",
    "\n",
    "The `think` method generates responses by sending user queries and the system prompt to the Ollama model.\n",
    "\n",
    "**Executing Tools**\n",
    "\n",
    "The `execute_tool` method determines which tool to use based on the model's response and executes it.\n",
    "\n",
    "**Main Method: Work**\n",
    "\n",
    "The `work` method is the primary entry point for processing user queries. It integrates the thinking and execution steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, tools, model_config):\n",
    "        \"\"\"\n",
    "        Initializes the agent with a list of tools and a model configuration.\n",
    "\n",
    "        Parameters:\n",
    "        tools (list): List of tool functions.\n",
    "        model_config (dict): Configuration for the model, including endpoint, model name, system prompt, etc.\n",
    "        \"\"\"\n",
    "        self.tools = tools\n",
    "        self.model_endpoint = model_config[\"model_endpoint\"]\n",
    "        self.model_name = model_config[\"model\"]\n",
    "        self.system_prompt = model_config[\"system_prompt\"]\n",
    "        self.temperature = model_config[\"temperature\"]\n",
    "        self.headers = model_config[\"headers\"]\n",
    "        self.stop = model_config[\"stop\"]\n",
    "        self.tool_descriptions = self.prepare_tools()\n",
    "\n",
    "    def prepare_tools(self):\n",
    "        \"\"\"\n",
    "        Prepares descriptions of the available tools.\n",
    "\n",
    "        Returns:\n",
    "        str: JSON-like formatted string describing the tools.\n",
    "        \"\"\"\n",
    "        tools_info = []\n",
    "\n",
    "        for tool in self.tools:\n",
    "            name = tool.__name__\n",
    "            docstring = (\n",
    "                tool.__doc__.strip() if tool.__doc__ else \"No description available.\"\n",
    "            )\n",
    "            parts = docstring.split(\"\\n\\n\", 1)\n",
    "            description = parts[0].split(\"\\n\")[0]\n",
    "            parameters_section = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "            param_dict = {}\n",
    "\n",
    "            if \"Parameters:\" in parameters_section:\n",
    "                parameters_lines = parameters_section.split(\"\\n\")\n",
    "                param_section_started = False\n",
    "\n",
    "                for line in parameters_lines:\n",
    "                    if \"Parameters:\" in line:\n",
    "                        param_section_started = True\n",
    "                        continue\n",
    "                    if not param_section_started or not line.strip():\n",
    "                        continue\n",
    "\n",
    "                    if \": \" in line:\n",
    "                        param_name_type_desc = line.split(\":\", 1)\n",
    "                        if len(param_name_type_desc) == 2:\n",
    "                            param_name_type, description = param_name_type_desc\n",
    "                            param_name = param_name_type.split(\"(\", 1)[0].strip()\n",
    "                            param_type = (\n",
    "                                param_name_type.split(\"(\", 1)[1].split(\")\", 1)[0]\n",
    "                                if \"(\" in param_name_type\n",
    "                                else \"str\"\n",
    "                            )\n",
    "                            param_dict[param_name] = {\n",
    "                                \"param_type\": param_type.strip(),\n",
    "                                \"description\": description.strip(),\n",
    "                                \"required\": True,\n",
    "                            }\n",
    "\n",
    "            tool_info = {\n",
    "                \"name\": name,\n",
    "                \"description\": description,\n",
    "                \"parameters\": param_dict,\n",
    "            }\n",
    "            tools_info.append(tool_info)\n",
    "\n",
    "        tools_description = \"\\n\".join(\n",
    "            [\n",
    "                f\"Use the function '{info['name']}' to: {info['description']}\\n{json.dumps(info, indent=2)}\"\n",
    "                for info in tools_info\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return tools_description\n",
    "\n",
    "    def think(self, prompt):\n",
    "        \"\"\"\n",
    "        Generates a response from the model based on the provided prompt.\n",
    "\n",
    "        Parameters:\n",
    "        prompt (str): The user query.\n",
    "\n",
    "        Returns:\n",
    "        dict: The response from the model.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"format\": \"json\",\n",
    "            \"prompt\": prompt,\n",
    "            \"system\": self.system_prompt.format(\n",
    "                tool_descriptions=self.tool_descriptions\n",
    "            ),\n",
    "            \"stream\": False,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"stop\": self.stop,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.model_endpoint, headers=self.headers, data=json.dumps(payload)\n",
    "            )\n",
    "            response_json = response.json()\n",
    "            response_dict = json.loads(response_json[\"response\"])\n",
    "\n",
    "            print(f\"\\n\\nResponse from model: {response_dict}\")\n",
    "            return response_dict\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error in invoking model! {str(e)}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def execute_tool(self, response):\n",
    "        \"\"\"\n",
    "        Executes the appropriate tool based on the model's response.\n",
    "\n",
    "        Parameters:\n",
    "        response (dict): The model's response containing tool_choice and tool_input.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        tool_choice = response.get(\"tool_choice\")\n",
    "        tool_input = response.get(\"tool_input\")\n",
    "\n",
    "        for tool in self.tools:\n",
    "            if tool.__name__ == tool_choice:\n",
    "                try:\n",
    "                    result = tool(tool_input)\n",
    "                    print(colored(result, \"cyan\"))\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        colored(f\"Error executing tool {tool_choice}: {str(e)}\", \"red\")\n",
    "                    )\n",
    "                return\n",
    "\n",
    "        print(colored(f\"Tool {tool_choice} not found or unsupported operation.\", \"red\"))\n",
    "\n",
    "    def work(self, prompt):\n",
    "        \"\"\"\n",
    "        The main method to process a user query, generate a response, and execute the appropriate tool.\n",
    "\n",
    "        Parameters:\n",
    "        prompt (str): The user query.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        response = self.think(prompt)\n",
    "        self.execute_tool(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "agent = Agent(tools=tools, model_config=ollama_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agent\n",
    "\n",
    "This section demonstrates how to run the agent with example prompts. We will use the `work` method of the `Agent` class to process different types of user queries, showcasing the agent's ability to select and execute the appropriate tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Basic Calculation\n",
    "\n",
    "In this example, we provide the agent with a mathematical query. The agent will determine the appropriate tool to use (in this case, `basic_calculator`) and execute it to provide the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Response from model: {'tool_choice': 'basic_calculator', 'tool_input': '{\"num1\": 5, \"operation\": \"add\", \"num2\": 5}'}\n",
      "\u001b[36m\n",
      "\n",
      "The answer is: 10.\n",
      "Calculated with basic_calculator.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is 5+5?\"\n",
    "agent.work(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: String Manipulation\n",
    "\n",
    "This example involves a string manipulation task where the user asks the agent to reverse a given string. The agent identifies and uses the `reverse_string` tool to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Response from model: {'tool_choice': 'reverse_string', 'tool_input': 'I want to be reversed!'}\n",
      "\u001b[36mThe reversed string is: !desrever eb ot tnaw I\n",
      "\n",
      "Executed using the reverse_string function.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Reverse this string: I want to be reversed!\"\n",
    "agent.work(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
