{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing Necessary Packages\n",
    "\n",
    "To install the necessary packages for this project, use the following pip command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: pyaudio in ./.conda/lib/python3.11/site-packages (0.2.14)\n",
      "Requirement already satisfied: faster-whisper in ./.conda/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: av==11.* in ./.conda/lib/python3.11/site-packages (from faster-whisper) (11.0.0)\n",
      "Requirement already satisfied: ctranslate2<5,>=4.0 in ./.conda/lib/python3.11/site-packages (from faster-whisper) (4.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.13 in ./.conda/lib/python3.11/site-packages (from faster-whisper) (0.24.3)\n",
      "Requirement already satisfied: tokenizers<0.16,>=0.13 in ./.conda/lib/python3.11/site-packages (from faster-whisper) (0.15.2)\n",
      "Requirement already satisfied: onnxruntime<2,>=1.14 in ./.conda/lib/python3.11/site-packages (from faster-whisper) (1.18.1)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.11/site-packages (from ctranslate2<5,>=4.0->faster-whisper) (69.5.1)\n",
      "Requirement already satisfied: pyyaml<7,>=5.3 in ./.conda/lib/python3.11/site-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.1)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.13->faster-whisper) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.13->faster-whisper) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.13->faster-whisper) (24.1)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.13->faster-whisper) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.13->faster-whisper) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.13->faster-whisper) (4.12.2)\n",
      "Requirement already satisfied: coloredlogs in ./.conda/lib/python3.11/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.conda/lib/python3.11/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (24.3.25)\n",
      "Requirement already satisfied: protobuf in ./.conda/lib/python3.11/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (5.27.2)\n",
      "Requirement already satisfied: sympy in ./.conda/lib/python3.11/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.12)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.conda/lib/python3.11/site-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper) (10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.conda/lib/python3.11/site-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pyaudio faster-whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Necessary Libraries\n",
    "\n",
    "In this step, we import all the required libraries for audio processing, handling queues, threading, and the Whisper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported all necessary libraries.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import queue\n",
    "import re\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "from faster_whisper import WhisperModel\n",
    "import pyaudio\n",
    "\n",
    "# Print statement for beginners to know the step completed\n",
    "print(\"Imported all necessary libraries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and Load the Whisper Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Audio Settings\n",
    "\n",
    "This section defines the settings for the Whisper model and audio configuration, including model type, processing options, and audio properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "\n",
    "# MODEL_TYPE: Defines the type of Whisper model to use. Options include \"small\", \"medium\", \"large\", etc.\n",
    "# Smaller models are faster but less accurate, while larger models are more accurate but require more resources.\n",
    "MODEL_TYPE = \"small\"\n",
    "\n",
    "# RUN_TYPE: Specifies whether the model should run on a CPU or GPU. Set to \"gpu\" for GPU acceleration if available.\n",
    "RUN_TYPE = \"cpu\"  # Change to \"gpu\" if you have a GPU available\n",
    "\n",
    "# For CPU usage:\n",
    "# NUM_WORKERS: Number of worker threads used by the model for CPU operations. More workers can speed up processing.\n",
    "NUM_WORKERS = 10\n",
    "\n",
    "# CPU_THREADS: Number of threads to use for CPU operations. This should ideally match the number of CPU cores available.\n",
    "CPU_THREADS = 4\n",
    "\n",
    "# For GPU usage:\n",
    "# GPU_DEVICE_INDICES: List of GPU indices to use. For example, [0, 1] will use the first two GPUs.\n",
    "GPU_DEVICE_INDICES = [0, 1, 2, 3]\n",
    "\n",
    "# VAD_FILTER: Voice Activity Detection filter flag. When True, the model will filter out non-speech audio segments.\n",
    "VAD_FILTER = True\n",
    "\n",
    "# Visualization (expected max number of characters for LENGTH_IN_SEC audio)\n",
    "# MAX_SENTENCE_CHARACTERS: The maximum number of characters expected in a single line of transcription.\n",
    "# This helps in formatting the display of transcribed text.\n",
    "MAX_SENTENCE_CHARACTERS = 80\n",
    "\n",
    "# Audio settings\n",
    "\n",
    "# STEP_IN_SEC: The length of each audio chunk in seconds. This defines the duration of audio data captured in one go.\n",
    "STEP_IN_SEC: int = 1\n",
    "\n",
    "# LENGTH_IN_SEC: Maximum duration of audio data to process at once. This sets the maximum length of audio data that will be processed together.\n",
    "LENGTH_IN_SEC: int = 6\n",
    "\n",
    "# NB_CHANNELS: The number of audio channels. 1 for mono, 2 for stereo.\n",
    "NB_CHANNELS = 1\n",
    "\n",
    "# RATE: The sample rate of the audio data (in Hz). Common rates include 16000 (16kHz) and 44100 (44.1kHz).\n",
    "RATE = 16000\n",
    "\n",
    "# CHUNK: The number of audio samples per frame. This typically matches the sample rate for 1 second of audio data.\n",
    "CHUNK = RATE\n",
    "\n",
    "## INPUT_DEVICE_ID\n",
    "INPUT_DEVICE_ID = 3\n",
    "\n",
    "# Queues to handle audio data\n",
    "\n",
    "# audio_queue: Queue to store audio data chunks captured from the microphone. These chunks are processed sequentially.\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# length_queue: Queue to store audio chunks that will be processed together. It helps manage the batch size of audio data.\n",
    "# maxsize is set to LENGTH_IN_SEC to limit the number of chunks held at once.\n",
    "length_queue = queue.Queue(maxsize=LENGTH_IN_SEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Load the Whisper Model\n",
    "\n",
    "This function initializes the Whisper model based on the specified settings. It can run on either CPU or GPU, depending on the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n",
      "Whisper model is ready to use.\n"
     ]
    }
   ],
   "source": [
    "# Function to create the Whisper model\n",
    "def create_whisper_model() -> WhisperModel:\n",
    "    if RUN_TYPE.lower() == \"gpu\":\n",
    "        whisper = WhisperModel(\n",
    "            MODEL_TYPE,\n",
    "            device=\"cuda\",\n",
    "            compute_type=\"float16\",\n",
    "            device_index=GPU_DEVICE_INDICES,\n",
    "            download_root=\"./models\",\n",
    "        )\n",
    "    elif RUN_TYPE.lower() == \"cpu\":\n",
    "        whisper = WhisperModel(\n",
    "            MODEL_TYPE,\n",
    "            device=\"cpu\",\n",
    "            compute_type=\"int8\",\n",
    "            num_workers=NUM_WORKERS,\n",
    "            cpu_threads=CPU_THREADS,\n",
    "            download_root=\"./models\",\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model type: {RUN_TYPE}\")\n",
    "\n",
    "    print(\"Loaded model\")\n",
    "    return whisper\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = create_whisper_model()\n",
    "print(\"Whisper model is ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transcription, Record and Process Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcription Function\n",
    "\n",
    "This function uses the Whisper model to transcribe audio data into text. It processes the audio, detects speech segments, and generates text along with language information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription function is set up.\n"
     ]
    }
   ],
   "source": [
    "# Function to transcribe audio using the Whisper model\n",
    "def execute_whisper_transcription(\n",
    "    model: WhisperModel, audio_data_array: np.ndarray, language_code: str = \"\"\n",
    ") -> Tuple[str, str, float]:\n",
    "    language_code = language_code.lower().strip()\n",
    "    segments, info = model.transcribe(\n",
    "        audio_data_array,\n",
    "        language=language_code if language_code != \"\" else None,\n",
    "        beam_size=5,\n",
    "        vad_filter=VAD_FILTER,\n",
    "        vad_parameters=dict(min_silence_duration_ms=500),\n",
    "    )\n",
    "    segments = [s.text for s in segments]\n",
    "    transcription = \" \".join(segments).strip()\n",
    "    return transcription, info.language, info.language_probability\n",
    "\n",
    "\n",
    "print(\"Transcription function is set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Recording Function\n",
    "\n",
    "This function captures audio data from the microphone in chunks and places it into a queue for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to record audio from the microphone\n",
    "def record_audio():\n",
    "    global running\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(\n",
    "        format=pyaudio.paInt16,\n",
    "        channels=NB_CHANNELS,\n",
    "        rate=RATE,\n",
    "        input=True,\n",
    "        frames_per_buffer=CHUNK,  # 1 second of audio\n",
    "        input_device_index=INPUT_DEVICE_ID,  # Specify the selected input device\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Microphone initialized, recording started...\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"TRANSCRIPTION\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    while running:\n",
    "        audio_data = b\"\"\n",
    "        for _ in range(STEP_IN_SEC):\n",
    "            chunk = stream.read(RATE)  # Read 1 second of audio data\n",
    "            audio_data += chunk\n",
    "\n",
    "        audio_queue.put(audio_data)  # Put the 1-second audio data into the queue\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    print(\"Microphone recording stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Processing Function\n",
    "\n",
    "This function processes the audio data from the queue, transcribes it, and outputs the transcribed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process audio and get transcription\n",
    "def process_audio(stats):\n",
    "    global running\n",
    "    while running or not audio_queue.empty():\n",
    "        if length_queue.qsize() >= LENGHT_IN_SEC:\n",
    "            with length_queue.mutex:\n",
    "                length_queue.queue.clear()\n",
    "                print()\n",
    "\n",
    "        try:\n",
    "            audio_data = audio_queue.get(timeout=1)\n",
    "        except queue.Empty:\n",
    "            continue\n",
    "\n",
    "        transcription_start_time = time.time()\n",
    "        length_queue.put(audio_data)\n",
    "\n",
    "        # Concatenate audio data in the length_queue\n",
    "        audio_data_to_process = b\"\"\n",
    "        for i in range(length_queue.qsize()):\n",
    "            # We index it so it won't get removed\n",
    "            audio_data_to_process += length_queue.queue[i]\n",
    "\n",
    "        try:\n",
    "            # Convert to NumPy array and normalize\n",
    "            audio_np = (\n",
    "                np.frombuffer(audio_data_to_process, np.int16).astype(np.float32)\n",
    "                / 255.0\n",
    "            )\n",
    "            transcription, language, language_probability = (\n",
    "                execute_whisper_transcription(model, audio_np)\n",
    "            )\n",
    "            transcription = re.sub(r\"\\[.*\\]\", \"\", transcription)\n",
    "            transcription = re.sub(r\"\\(.*\\)\", \"\", transcription)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            transcription = \"Error\"\n",
    "\n",
    "        transcription_end_time = time.time()\n",
    "\n",
    "        # Display transcription\n",
    "        transcription_to_visualize = transcription.ljust(MAX_SENTENCE_CHARACTERS, \" \")\n",
    "        transcription_postprocessing_end_time = time.time()\n",
    "\n",
    "        sys.stdout.write(\"\\033[K\" + transcription_to_visualize + \"\\r\")\n",
    "\n",
    "        audio_queue.task_done()\n",
    "\n",
    "        overall_elapsed_time = (\n",
    "            transcription_postprocessing_end_time - transcription_start_time\n",
    "        )\n",
    "        transcription_elapsed_time = transcription_end_time - transcription_start_time\n",
    "        postprocessing_elapsed_time = (\n",
    "            transcription_postprocessing_end_time - transcription_end_time\n",
    "        )\n",
    "        stats[\"overall\"].append(overall_elapsed_time)\n",
    "        stats[\"transcription\"].append(transcription_elapsed_time)\n",
    "        stats[\"postprocessing\"].append(postprocessing_elapsed_time)\n",
    "\n",
    "    print(\"Audio processing stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the input device index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID 1: External Microphone - Channels: 1\n",
      "Device ID 3: MacBook Pro Microphone - Channels: 1\n",
      "Device ID 5: Gabriel’s iPhone Microphone - Channels: 1\n",
      "Device ID 6: Microsoft Teams Audio - Channels: 2\n"
     ]
    }
   ],
   "source": [
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# List all audio input devices\n",
    "input_devices = []\n",
    "for i in range(audio.get_device_count()):\n",
    "    device_info = audio.get_device_info_by_index(i)\n",
    "    if device_info[\"maxInputChannels\"] > 0:\n",
    "        input_devices.append((i, device_info[\"name\"], device_info[\"maxInputChannels\"]))\n",
    "        print(\n",
    "            f\"Device ID {i}: {device_info['name']} - Channels: {device_info['maxInputChannels']}\"\n",
    "        )\n",
    "\n",
    "# Terminate PyAudio instance (to be reinitialized later in the recording function)\n",
    "audio.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Device ID: 3, Channels: 1\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user to select a device by entering the device ID\n",
    "INPUT_DEVICE_ID = int(\n",
    "    input(\"Enter the Device ID of the input device you want to use: \")\n",
    ")\n",
    "\n",
    "# Retrieve the number of channels for the selected device\n",
    "selected_device_info = next(\n",
    "    (device for device in input_devices if device[0] == INPUT_DEVICE_ID), None\n",
    ")\n",
    "if selected_device_info:\n",
    "    selected_channels = selected_device_info[2]\n",
    "    print(f\"Selected Device ID: {INPUT_DEVICE_ID}, Channels: {selected_channels}\")\n",
    "else:\n",
    "    print(\"Invalid Device ID\")\n",
    "\n",
    "# Set global channel count based on selected device\n",
    "NB_CHANNELS = selected_channels if selected_device_info else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running the Audio Processing System\n",
    "\n",
    "This final cell sets up the audio recording and processing threads, and handles the clean shutdown of these threads upon interruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio recording and processing started. Press 'Stop' or interrupt the kernel to stop.\n",
      "--------------------------------------------------------------------------------\n",
      "Microphone initialized, recording started...\n",
      "--------------------------------------------------------------------------------\n",
      "TRANSCRIPTION\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[KAnd this is the end of this video.  Thank you for watching.  I hope you enjoyed this video.  I'll see you in the next video.  Bye.\n",
      "\u001b[Kand talk to those at the center of the problem to find out what can be done.    \n",
      "Stopping...\n",
      "Microphone recording stopped.                                                      \n",
      "\u001b[K                                                                                \n",
      "Audio processing stopped.                                                          \n",
      "Stopped.\n",
      "Number of processed chunks:  20\n",
      "Overall time: avg: 1.2311s, std: 0.9780s\n",
      "Transcription time: avg: 1.2311s, std: 0.9780s\n",
      "Postprocessing time: avg: 0.0000s, std: 0.0000s\n",
      "The average latency is 2.2311s\n"
     ]
    }
   ],
   "source": [
    "# Flag to control the running state of threads\n",
    "running = True\n",
    "\n",
    "# Initialize statistics dictionary\n",
    "stats: Dict[str, List[float]] = {\n",
    "    \"overall\": [],\n",
    "    \"transcription\": [],\n",
    "    \"postprocessing\": [],\n",
    "}\n",
    "\n",
    "# Start recording and processing threads\n",
    "producer = threading.Thread(target=record_audio)\n",
    "producer.start()\n",
    "\n",
    "consumer = threading.Thread(target=process_audio, args=(stats,))\n",
    "consumer.start()\n",
    "\n",
    "print(\n",
    "    \"Audio recording and processing started. Press 'Stop' or interrupt the kernel to stop.\"\n",
    ")\n",
    "\n",
    "# This block is to ensure proper shutdown of threads\n",
    "try:\n",
    "    producer.join()\n",
    "    consumer.join()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping...\")\n",
    "    running = False\n",
    "    producer.join()\n",
    "    consumer.join()\n",
    "    print(\"Stopped.\")\n",
    "    # Print statistics\n",
    "    print(\"Number of processed chunks: \", len(stats[\"overall\"]))\n",
    "    print(\n",
    "        f\"Overall time: avg: {np.mean(stats['overall']):.4f}s, std: {np.std(stats['overall']):.4f}s\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Transcription time: avg: {np.mean(stats['transcription']):.4f}s, std: {np.std(stats['transcription']):.4f}s\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Postprocessing time: avg: {np.mean(stats['postprocessing']):.4f}s, std: {np.std(stats['postprocessing']):.4f}s\"\n",
    "    )\n",
    "    print(f\"The average latency is {np.mean(stats['overall']) + STEP_IN_SEC:.4f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
