{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LLM Agents with LLama3 and LangGraph\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, you will learn how to build and deploy LLM (Large Language Model) agents using LLama3 and LangGraph. These agents will be capable of processing natural language inputs, generating comprehensive plans, and handling complex workflows. By the end of this notebook, you will understand how to:\n",
    "\n",
    "1. Set up and configure a language model using LLama3.\n",
    "2. Define and manage the state for agents in a workflow.\n",
    "3. Implement and customize agent classes for specific tasks.\n",
    "4. Construct and compile a workflow graph using LangGraph.\n",
    "5. Execute the workflow and handle outputs effectively.\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "Before diving into the implementation, it's essential to understand some basic concepts:\n",
    "\n",
    "- **LLM (Large Language Model):** A machine learning model trained on vast amounts of text data to understand and generate human-like language. LLama3 is an example of such a model.\n",
    "\n",
    "- **Agent:** A software component that interacts with the LLM to perform specific tasks, such as generating responses or processing information. In this notebook, we implement agents that can handle various aspects of a workflow.\n",
    "\n",
    "- **State:** A shared data structure that stores the context and data required by agents. The state is critical for maintaining continuity and passing information between different parts of the workflow.\n",
    "\n",
    "- **Workflow Graph:** A structured representation of the workflow, where nodes represent agents and edges define the flow of information and control. LangGraph is used to construct and manage these workflow graphs.\n",
    "\n",
    "- **Prompt Engineering:** The process of crafting prompts that guide the LLM's responses. Proper prompt engineering is crucial for ensuring the model generates relevant and accurate outputs.\n",
    "\n",
    "Understanding these concepts will provide a solid foundation as we proceed with the practical implementation of LLM agents and workflows in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "Before we dive into building our agent, we need to set up the necessary environment. This involves installing required packages and ensuring our Python environment is ready for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: termcolor in ./.conda/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: langgraph in ./.conda/lib/python3.11/site-packages (0.1.15)\n",
      "Requirement already satisfied: pyaudio in ./.conda/lib/python3.11/site-packages (0.2.14)\n",
      "Requirement already satisfied: faster-whisper in ./.conda/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2.22 in ./.conda/lib/python3.11/site-packages (from langgraph) (0.2.23)\n",
      "Requirement already satisfied: av==11.* in ./.conda/lib/python3.11/site-packages (from faster-whisper) (11.0.0)\n",
      "Requirement already satisfied: ctranslate2<5,>=4.0 in ./.conda/lib/python3.11/site-packages (from faster-whisper) (4.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.13 in ./.conda/lib/python3.11/site-packages (from faster-whisper) (0.24.3)\n",
      "Requirement already satisfied: tokenizers<0.16,>=0.13 in ./.conda/lib/python3.11/site-packages (from faster-whisper) (0.15.2)\n",
      "Requirement already satisfied: onnxruntime<2,>=1.14 in ./.conda/lib/python3.11/site-packages (from faster-whisper) (1.18.1)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.11/site-packages (from ctranslate2<5,>=4.0->faster-whisper) (69.5.1)\n",
      "Requirement already satisfied: numpy in ./.conda/lib/python3.11/site-packages (from ctranslate2<5,>=4.0->faster-whisper) (1.26.4)\n",
      "Requirement already satisfied: pyyaml<7,>=5.3 in ./.conda/lib/python3.11/site-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.1)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.13->faster-whisper) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.13->faster-whisper) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.13->faster-whisper) (24.1)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.13->faster-whisper) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.13->faster-whisper) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.13->faster-whisper) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2.22->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2.22->langgraph) (0.1.93)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2.22->langgraph) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2.22->langgraph) (8.5.0)\n",
      "Requirement already satisfied: coloredlogs in ./.conda/lib/python3.11/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.conda/lib/python3.11/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (24.3.25)\n",
      "Requirement already satisfied: protobuf in ./.conda/lib/python3.11/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (5.27.2)\n",
      "Requirement already satisfied: sympy in ./.conda/lib/python3.11/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.12)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.22->langgraph) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langgraph) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.22->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.22->langgraph) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2024.7.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.conda/lib/python3.11/site-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.conda/lib/python3.11/site-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install termcolor for colored terminal outputs\n",
    "%pip install termcolor langgraph pyaudio faster-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from termcolor import colored\n",
    "import json\n",
    "import requests\n",
    "from faster_whisper import WhisperModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Configuration\n",
    "\n",
    "In this section, we set up the configuration for the Ollama model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Ollama Model\n",
    "\n",
    "The `setup_ollama_model` function configures the model settings, including the endpoint, model name, system prompt, and other parameters. This setup is essential for initializing the model with the correct configuration, ensuring it can process queries and utilize the tools effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ollama_model(model, temperature=0, stop=None):\n",
    "    \"\"\"\n",
    "    Sets up the Ollama model configuration.\n",
    "\n",
    "    Parameters:\n",
    "    model (str): The name of the model to use.\n",
    "    temperature (float): The temperature setting for the model.\n",
    "    stop (str): The stop token for the model.\n",
    "\n",
    "    Returns:\n",
    "    dict: Configuration for the Ollama model.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model_endpoint\": \"http://localhost:11434/api/generate\",\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"stop\": stop,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example configuration\n",
    "ollama_config = setup_ollama_model(model=\"llama3:instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "\n",
    "# MODEL_TYPE: Defines the type of Whisper model to use. Options include \"small\", \"medium\", \"large\", etc.\n",
    "# Smaller models are faster but less accurate, while larger models are more accurate but require more resources.\n",
    "MODEL_TYPE = \"small\"\n",
    "\n",
    "# RUN_TYPE: Specifies whether the model should run on a CPU or GPU. Set to \"gpu\" for GPU acceleration if available.\n",
    "RUN_TYPE = \"cpu\"  # Change to \"gpu\" if you have a GPU available\n",
    "\n",
    "# For CPU usage:\n",
    "# NUM_WORKERS: Number of worker threads used by the model for CPU operations. More workers can speed up processing.\n",
    "NUM_WORKERS = 10\n",
    "\n",
    "# CPU_THREADS: Number of threads to use for CPU operations. This should ideally match the number of CPU cores available.\n",
    "CPU_THREADS = 4\n",
    "\n",
    "# For GPU usage:\n",
    "# GPU_DEVICE_INDICES: List of GPU indices to use. For example, [0, 1] will use the first two GPUs.\n",
    "GPU_DEVICE_INDICES = [0, 1, 2, 3]\n",
    "\n",
    "# VAD_FILTER: Voice Activity Detection filter flag. When True, the model will filter out non-speech audio segments.\n",
    "VAD_FILTER = True\n",
    "\n",
    "# Visualization (expected max number of characters for LENGTH_IN_SEC audio)\n",
    "# MAX_SENTENCE_CHARACTERS: The maximum number of characters expected in a single line of transcription.\n",
    "# This helps in formatting the display of transcribed text.\n",
    "MAX_SENTENCE_CHARACTERS = 80\n",
    "\n",
    "# Audio settings\n",
    "\n",
    "# STEP_IN_SEC: The length of each audio chunk in seconds. This defines the duration of audio data captured in one go.\n",
    "STEP_IN_SEC: int = 1\n",
    "\n",
    "# LENGTH_IN_SEC: Maximum duration of audio data to process at once. This sets the maximum length of audio data that will be processed together.\n",
    "LENGTH_IN_SEC: int = 6\n",
    "\n",
    "# NB_CHANNELS: The number of audio channels. 1 for mono, 2 for stereo.\n",
    "NB_CHANNELS = 1\n",
    "\n",
    "# RATE: The sample rate of the audio data (in Hz). Common rates include 16000 (16kHz) and 44100 (44.1kHz).\n",
    "RATE = 16000\n",
    "\n",
    "# CHUNK: The number of audio samples per frame. This typically matches the sample rate for 1 second of audio data.\n",
    "CHUNK = RATE\n",
    "\n",
    "## INPUT_DEVICE_ID\n",
    "INPUT_DEVICE_ID = 3\n",
    "\n",
    "# Queues to handle audio data\n",
    "\n",
    "# audio_queue: Queue to store audio data chunks captured from the microphone. These chunks are processed sequentially.\n",
    "# audio_queue = queue.Queue()\n",
    "\n",
    "# # length_queue: Queue to store audio chunks that will be processed together. It helps manage the batch size of audio data.\n",
    "# # maxsize is set to LENGTH_IN_SEC to limit the number of chunks held at once.\n",
    "# length_queue = queue.Queue(maxsize=LENGHT_IN_SEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper model is ready to use.\n"
     ]
    }
   ],
   "source": [
    "# Function to create the Whisper model\n",
    "def create_whisper_model() -> WhisperModel:\n",
    "    if RUN_TYPE.lower() == \"gpu\":\n",
    "        whisper = WhisperModel(\n",
    "            MODEL_TYPE,\n",
    "            device=\"cuda\",\n",
    "            compute_type=\"float16\",\n",
    "            device_index=GPU_DEVICE_INDICES,\n",
    "            download_root=\"./models\",\n",
    "        )\n",
    "    elif RUN_TYPE.lower() == \"cpu\":\n",
    "        whisper = WhisperModel(\n",
    "            MODEL_TYPE,\n",
    "            device=\"cpu\",\n",
    "            compute_type=\"int8\",\n",
    "            num_workers=NUM_WORKERS,\n",
    "            cpu_threads=CPU_THREADS,\n",
    "            download_root=\"./models\",\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model type: {RUN_TYPE}\")\n",
    "\n",
    "    print(\"Loaded model\")\n",
    "    return whisper\n",
    "\n",
    "\n",
    "# Load the model\n",
    "print(\"Whisper model is ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Defining the Agent Graph State\n",
    "\n",
    "In this step, we define the structure of the state that our agents will use to store and communicate information. This state acts as a shared memory that different components of the system can access and modify. We use the `TypedDict` from the `typing` module to define the expected structure and types of data within the state. This helps ensure consistency and correctness when accessing or updating the state, making it easier to manage complex workflows and data dependencies.\n",
    "\n",
    "The `AgentGraphState` class includes fields for the research question, responses from the planner agent, and any final outputs or end states. The `get_agent_graph`_state function is used to retrieve specific parts of the state based on a key, facilitating modular and reusable access to the state data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Define the state object for the agent graph\n",
    "class AgentGraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    This class defines the structure of the agent graph state.\n",
    "    \n",
    "    Attributes:\n",
    "    research_question (str): The main research question the agent is working on.\n",
    "    planner_response (list): A list to store responses from the planner agent.\n",
    "    end_chain (list): A list to store the final outputs or end states.\n",
    "    \"\"\"\n",
    "    research_question: str\n",
    "    transcription_response: Annotated[list, add_messages]\n",
    "    end_chain: Annotated[list, add_messages]\n",
    "\n",
    "# Function to retrieve specific parts of the agent state\n",
    "def get_agent_graph_state(state: AgentGraphState, state_key: str):\n",
    "    \"\"\"\n",
    "    Retrieves specific parts of the agent state based on the provided key.\n",
    "    \n",
    "    Parameters:\n",
    "    state (AgentGraphState): The current state of the agent.\n",
    "    state_key (str): The key indicating which part of the state to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "    list or None: The requested state data or None if the key is not recognized.\n",
    "    \"\"\"\n",
    "    if state_key == \"transcription_all\":\n",
    "        return state[\"transcription_response\"]\n",
    "    elif state_key == \"transcription_latest\":\n",
    "        return state[\"transcription_response\"][-1] if state[\"transcription_response\"] else []\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Initial state setup\n",
    "state = {\n",
    "    \"research_question\": \"\",\n",
    "    \"transcription_response\": [],\n",
    "    \"end_chain\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agent Class Definition\n",
    "\n",
    "In this section, we define the `Agent` class, which serves as a base class for different types of agents in our system. An agent is a component that interacts with the language model to perform specific tasks, such as generating responses or processing information. The `Agent` class manages the configuration and state associated with the language model, allowing for easy setup and reuse of model configurations across different agents.\n",
    "\n",
    "The class includes methods for initializing the agent with a specific model configuration and updating the agent's state. The state encapsulates the context or memory of the agent, enabling it to maintain continuity across interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state: AgentGraphState, model_config: dict):\n",
    "        \"\"\"\n",
    "        Initializes the agent with a state and model configuration.\n",
    "\n",
    "        Parameters:\n",
    "        state (AgentGraphState): The initial state of the agent, containing necessary context and data.\n",
    "        model_config (dict): Configuration settings for the model, including endpoint, model name, temperature, etc.\n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        self.model_endpoint = model_config.get(\"model_endpoint\")\n",
    "        self.model_name = model_config.get(\"model\")\n",
    "        self.temperature = model_config.get(\n",
    "            \"temperature\", 0\n",
    "        )  # Default temperature is 0\n",
    "        self.headers = model_config.get(\"headers\", {\"Content-Type\": \"application/json\"})\n",
    "        self.stop = model_config.get(\"stop\")\n",
    "\n",
    "    def update_state(self, key: str, value: any):\n",
    "        \"\"\"\n",
    "        Updates the agent's state with a new key-value pair.\n",
    "\n",
    "        Parameters:\n",
    "        key (str): The key in the state dictionary to update.\n",
    "        value (any): The new value to associate with the specified key.\n",
    "        \"\"\"\n",
    "        if key in self.state:\n",
    "            self.state[key] = value\n",
    "        else:\n",
    "            print(f\"Warning: Attempting to update a non-existing state key '{key}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Utility Functions\n",
    "\n",
    "Utility functions are auxiliary functions that assist with various common tasks within the notebook. They help keep the codebase clean and modular by encapsulating frequently used logic in separate functions. In this case, we have two utility functions: `check_for_content` and `get_current_utc_datetime`.\n",
    "\n",
    "- `check_for_content`: This function checks if a variable has a content attribute and returns its value if it exists. This is useful for handling different data types that may or may not have a content attribute.\n",
    "- `get_current_utc_datetime`: This function returns the current date and time in UTC format. This can be useful for timestamping events or logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "# Check if an attribute of the state dict has content\n",
    "def check_for_content(var):\n",
    "    \"\"\"\n",
    "    Checks if the provided variable has a 'content' attribute and returns it.\n",
    "\n",
    "    Parameters:\n",
    "    var (Any): The variable to check.\n",
    "\n",
    "    Returns:\n",
    "    Any: The 'content' attribute if it exists, otherwise the original variable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return var.content\n",
    "    except AttributeError:\n",
    "        return var\n",
    "\n",
    "\n",
    "# Get the current date and time in UTC\n",
    "def get_current_utc_datetime():\n",
    "    \"\"\"\n",
    "    Returns the current date and time in UTC.\n",
    "\n",
    "    Returns:\n",
    "    str: The current date and time in UTC, formatted as 'YYYY-MM-DD HH:MM:SS UTC'.\n",
    "    \"\"\"\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    return now_utc.strftime(\"%Y-%m-%d %H:%M:%S UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. AudioAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import os\n",
    "import wave\n",
    "import pyaudio\n",
    "import logging\n",
    "\n",
    "class AudioAgent(Agent):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state: AgentGraphState,\n",
    "        listen_filename: str = \"mp3_audio_files/tmp_listen.wav\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicializa o agente de transcriÃ§Ã£o de Ã¡udio com os parÃ¢metros especificados.\n",
    "\n",
    "        Args:\n",
    "            api_key: A chave da API para o OpenAI.\n",
    "            listen_filename: O nome do arquivo para armazenar o Ã¡udio gravado.\n",
    "            run_local: Se deve executar o modelo localmente.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            state, model_config={}\n",
    "        )\n",
    "        self.listen_filename = listen_filename\n",
    "        self.recording = False\n",
    "        self.record_lock = threading.Lock()\n",
    "        self.model = create_whisper_model()\n",
    "\n",
    "    def listen(self) -> str:\n",
    "        \"\"\"\n",
    "        Grava e transcreve o Ã¡udio usando a API do OpenAI.\n",
    "\n",
    "        Returns:\n",
    "        str: O texto transcrito do Ã¡udio.\n",
    "        \"\"\"\n",
    "        thread = threading.Thread(target=self.record_audio)\n",
    "        input(\"Press ENTER to START recording...\")\n",
    "        with self.record_lock:\n",
    "            self.recording = True\n",
    "        thread.start()\n",
    "        input(\"Press ENTER to STOP recording...\")\n",
    "        with self.record_lock:\n",
    "            self.recording = False\n",
    "        thread.join()\n",
    "\n",
    "        transcription = \"\"\n",
    "        try:\n",
    "            with open(self.listen_filename, \"rb\") as audio_file:\n",
    "                segments, info = self.model.transcribe(\n",
    "                    self.listen_filename, beam_size=5\n",
    "                )\n",
    "\n",
    "                print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "                for segment in segments:\n",
    "                    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "                    transcription += segment.text + \" \"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed when trying to transcript: {e}\")\n",
    "        finally:\n",
    "            if os.path.exists(self.listen_filename):\n",
    "                os.remove(self.listen_filename)\n",
    "            return transcription\n",
    "\n",
    "\n",
    "# Function to record audio from the microphone\n",
    "def record_audio():\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(\n",
    "        format=pyaudio.paInt16,\n",
    "        channels=NB_CHANNELS,\n",
    "        rate=RATE,\n",
    "        input=True,\n",
    "        frames_per_buffer=CHUNK,  # 1 second of audio\n",
    "        input_device_index=INPUT_DEVICE_ID,  # Specify the selected input device\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Microphone initialized, recording started...\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"TRANSCRIPTION\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    while self.recording:\n",
    "        audio_data = b\"\"\n",
    "        for _ in range(STEP_IN_SEC):\n",
    "            chunk = stream.read(RATE)  # Read 1 second of audio data\n",
    "            audio_data += chunk\n",
    "\n",
    "        audio_queue.put(audio_data)  # Put the 1-second audio data into the queue\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    print(\"Microphone recording stopped.\")\n",
    "\n",
    "    def record_audio(self) -> None:\n",
    "        \"\"\"\n",
    "        Grava Ã¡udio do microfone e salva em um arquivo.\n",
    "        \"\"\"\n",
    "        chunk = 1024\n",
    "        sample_format = pyaudio.paInt16\n",
    "        channels = 1\n",
    "        fs = 44100\n",
    "        p = pyaudio.PyAudio()\n",
    "        stream = p.open(\n",
    "            format=sample_format,\n",
    "            channels=channels,\n",
    "            rate=fs,\n",
    "            frames_per_buffer=chunk,\n",
    "            input=True,\n",
    "        )\n",
    "        frames = []\n",
    "\n",
    "        try:\n",
    "            while self.recording:\n",
    "                data = stream.read(chunk)\n",
    "                frames.append(data)\n",
    "        finally:\n",
    "            stream.stop_stream()\n",
    "            stream.close()\n",
    "            p.terminate()\n",
    "\n",
    "        try:\n",
    "            with wave.open(self.listen_filename, \"wb\") as wf:\n",
    "                wf.setnchannels(channels)\n",
    "                wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "                wf.setframerate(fs)\n",
    "                wf.writeframes(b\"\".join(frames))\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed when trying to save audio: {e}\")\n",
    "\n",
    "    def invoke(\n",
    "        self,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Generates a response from the model based on the provided prompt.\n",
    "\n",
    "        Parameters:\n",
    "        research_question (str): The research question the planner agent needs to address.\n",
    "        prompt (str): The template used to generate the system prompt.\n",
    "        feedback (callable or str): Feedback received to adjust the planning process.\n",
    "\n",
    "        Returns:\n",
    "        dict: The updated state of the agent after processing the response.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            transcription = self.listen()\n",
    "\n",
    "            response_formatted = HumanMessage(content=transcription)\n",
    "            self.update_state(\"transcription_response\", response_formatted)\n",
    "            print(colored(f\"Audio ðŸ‘©ðŸ¿â€ðŸ’»: {response_formatted}\", \"cyan\"))\n",
    "            return self.state\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error in invoking model! {str(e)}\")\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. End Node\n",
    "\n",
    "The `EndNodeAgent` class is a specialized agent that marks the conclusion of the workflow in the agent graph. It extends the base `Agent` class and is primarily responsible for updating the state to indicate the end of the process. This agent is useful for workflows that require a clear termination point, ensuring that the system knows when all processing is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndNodeAgent(Agent):\n",
    "    def invoke(self) -> AgentGraphState:\n",
    "        \"\"\"\n",
    "        Marks the end of the workflow by updating the state.\n",
    "\n",
    "        This method updates the 'end_chain' key in the state to signify that\n",
    "        the workflow has reached its conclusion. It can be used to perform any\n",
    "        finalization tasks or simply to denote that the agent has completed its role.\n",
    "\n",
    "        Returns:\n",
    "        AgentGraphState: The updated state of the agent.\n",
    "        \"\"\"\n",
    "        self.update_state(\"end_chain\", \"end_chain\")\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating and Compiling the Agent Graph\n",
    "\n",
    "In this section, we define the structure and flow of the agent-based system using the `StateGraph` class from the `langgraph` library. The graph consists of nodes, each representing a specific agent, and edges, which define the flow or sequence of operations. This setup enables the modeling of complex workflows where different agents can interact and pass information.\n",
    "\n",
    "- **`create_graph`:** This function initializes the `StateGraph` with a specific state structure (`AgentGraphState`). It then adds nodes for the `PlannerAgent` and `EndNodeAgent`, specifying the operations these agents should perform. The function sets the \"planner\" node as the entry point and the \"end\" node as the finish point, with an edge connecting them to define the workflow sequence.\n",
    "- **`compile_workflow`:** This function compiles the defined graph into a workflow that can be executed. The compiled workflow manages the execution of the nodes in the defined order, handling the flow of data and control through the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def create_graph() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Creates and configures the state graph for the agent workflow.\n",
    "\n",
    "    This function initializes the graph, adds the necessary nodes (agents), and\n",
    "    sets up the edges defining the flow of the workflow.\n",
    "\n",
    "    Returns:\n",
    "    StateGraph: The configured state graph for the workflow.\n",
    "    \"\"\"\n",
    "    graph = StateGraph(AgentGraphState)\n",
    "    # graph.add_node(\n",
    "    #     \"planner\",\n",
    "    #     lambda state: PlannerAgent(\n",
    "    #         state=state,\n",
    "    #         model_config=ollama_config,\n",
    "    #     ).invoke(\n",
    "    #         research_question=state[\"research_question\"],\n",
    "    #         feedback=lambda: get_agent_graph_state(\n",
    "    #             state=state, state_key=\"reviewer_latest\"\n",
    "    #         ),\n",
    "    #         prompt=planner_prompt_template,\n",
    "    #     ),\n",
    "    # )\n",
    "\n",
    "    graph.add_node(\n",
    "        \"audio\",\n",
    "        lambda state: AudioAgent(\n",
    "            state=state,\n",
    "        ).invoke(\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    graph.add_node(\n",
    "        \"end\",\n",
    "        lambda state: EndNodeAgent(\n",
    "            state=state,\n",
    "            model_config=ollama_config,\n",
    "        ).invoke(),\n",
    "    )\n",
    "\n",
    "    # Set the entry and finish points for the workflow\n",
    "    graph.set_entry_point(\"audio\")\n",
    "    graph.set_finish_point(\"end\")\n",
    "\n",
    "    # Define the flow of the graph\n",
    "    graph.add_edge(\"audio\", \"end\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def compile_workflow(graph: StateGraph):\n",
    "    \"\"\"\n",
    "    Compiles the state graph into an executable workflow.\n",
    "\n",
    "    This function compiles the graph, enabling the defined nodes and edges to\n",
    "    be executed in sequence as per the workflow's logic.\n",
    "\n",
    "    Parameters:\n",
    "    graph (StateGraph): The state graph defining the workflow.\n",
    "\n",
    "    Returns:\n",
    "    Any: The compiled workflow ready for execution.\n",
    "    \"\"\"\n",
    "    workflow = graph.compile()\n",
    "    return workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Workflow\n",
    "\n",
    "In this final section, we execute the workflow defined in the agent graph. We start by creating the graph using the `create_graph` function and then compiling it into an executable workflow with `compile_workflow`. The workflow is then run with specific inputs and configurations.\n",
    "\n",
    "- **`graph = create_graph()`:** This initializes the graph structure, including all nodes and edges as defined previously.\n",
    "- **`workflow = compile_workflow(graph)`:** This compiles the graph into a runnable workflow, preparing it for execution.\n",
    "- **`iterations = 10`:** This variable sets the recursion limit for the workflow, determining how many iterations the workflow should allow.\n",
    "- **`verbose = True`:** If set to `True`, the system will print detailed information about each state change during the workflow execution.\n",
    "- **`query = \"Who's the president of the USA?\"`:** The research question provided as input to the workflow, which the planner agent will process.\n",
    "- **`dict_inputs = {\"research_question\": query}`:** A dictionary containing the initial inputs to the workflow, including the research question.\n",
    "- **`limit = {\"recursion_limit\": iterations}`:** This sets the limit for the number of iterations, preventing infinite loops or excessive processing.\n",
    "\n",
    "The loop iterates over the events generated by the workflow, printing the state at each step if `verbose` is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph and workflow created.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "AudioAgent.__init__() got an unexpected keyword argument 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m limit \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: iterations}\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Execute the workflow and print state changes\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdict_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mState Dictionary:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/redhat/ai/llm-agents/.conda/lib/python3.11/site-packages/langgraph/pregel/__init__.py:948\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 948\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m~/redhat/ai/llm-agents/.conda/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1349\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1348\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1354\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/redhat/ai/llm-agents/.conda/lib/python3.11/site-packages/langgraph/pregel/executor.py:60\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m~/redhat/ai/llm-agents/.conda/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/redhat/ai/llm-agents/.conda/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/redhat/ai/llm-agents/.conda/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/redhat/ai/llm-agents/.conda/lib/python3.11/site-packages/langgraph/pregel/retry.py:25\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     23\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/redhat/ai/llm-agents/.conda/lib/python3.11/site-packages/langchain_core/runnables/base.py:2873\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2869\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   2870\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2871\u001b[0m )\n\u001b[1;32m   2872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2873\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2875\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/redhat/ai/llm-agents/.conda/lib/python3.11/site-packages/langgraph/utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[93], line 30\u001b[0m, in \u001b[0;36mcreate_graph.<locals>.<lambda>\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     13\u001b[0m graph \u001b[38;5;241m=\u001b[39m StateGraph(AgentGraphState)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# graph.add_node(\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     \"planner\",\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     lambda state: PlannerAgent(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     ),\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     28\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_node(\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m state: \u001b[43mAudioAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m     33\u001b[0m     ),\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_node(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m state: EndNodeAgent(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     )\u001b[38;5;241m.\u001b[39minvoke(),\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Set the entry and finish points for the workflow\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: AudioAgent.__init__() got an unexpected keyword argument 'state'"
     ]
    }
   ],
   "source": [
    "# Create the graph and compile the workflow\n",
    "graph = create_graph()\n",
    "workflow = compile_workflow(graph)\n",
    "print(\"Graph and workflow created.\")\n",
    "\n",
    "# Define workflow parameters\n",
    "iterations = 10\n",
    "verbose = True\n",
    "query = \"Who's the president of the USA?\"\n",
    "dict_inputs = {\"research_question\": query}\n",
    "limit = {\"recursion_limit\": iterations}\n",
    "\n",
    "# Execute the workflow and print state changes\n",
    "for event in workflow.stream(dict_inputs, limit):\n",
    "    if verbose:\n",
    "        print(\"\\nState Dictionary:\", event)\n",
    "    else:\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
