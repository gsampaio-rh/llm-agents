{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/m-bain/whisperx.git\n",
      "  Cloning https://github.com/m-bain/whisperx.git to /private/var/folders/8r/2hn86n416n58v77nhrr2_mhw0000gn/T/pip-req-build-dayj0auq\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/m-bain/whisperx.git /private/var/folders/8r/2hn86n416n58v77nhrr2_mhw0000gn/T/pip-req-build-dayj0auq\n",
      "  Resolved https://github.com/m-bain/whisperx.git to commit 58f00339af7dcc9705ef49d97a1f40764b7cf555\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=2 in ./.conda/lib/python3.11/site-packages (from whisperx==3.1.1) (2.4.0)\n",
      "Requirement already satisfied: torchaudio>=2 in ./.conda/lib/python3.11/site-packages (from whisperx==3.1.1) (2.4.0)\n",
      "Requirement already satisfied: faster-whisper==1.0.0 in ./.conda/lib/python3.11/site-packages (from whisperx==3.1.1) (1.0.0)\n",
      "Requirement already satisfied: transformers in ./.conda/lib/python3.11/site-packages (from whisperx==3.1.1) (4.39.3)\n",
      "Requirement already satisfied: pandas in ./.conda/lib/python3.11/site-packages (from whisperx==3.1.1) (2.2.2)\n",
      "Requirement already satisfied: setuptools>=65 in ./.conda/lib/python3.11/site-packages (from whisperx==3.1.1) (69.5.1)\n",
      "Requirement already satisfied: nltk in ./.conda/lib/python3.11/site-packages (from whisperx==3.1.1) (3.8.1)\n",
      "Requirement already satisfied: pyannote.audio==3.1.1 in ./.conda/lib/python3.11/site-packages (from whisperx==3.1.1) (3.1.1)\n",
      "Requirement already satisfied: av==11.* in ./.conda/lib/python3.11/site-packages (from faster-whisper==1.0.0->whisperx==3.1.1) (11.0.0)\n",
      "Requirement already satisfied: ctranslate2<5,>=4.0 in ./.conda/lib/python3.11/site-packages (from faster-whisper==1.0.0->whisperx==3.1.1) (4.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.13 in ./.conda/lib/python3.11/site-packages (from faster-whisper==1.0.0->whisperx==3.1.1) (0.24.3)\n",
      "Requirement already satisfied: tokenizers<0.16,>=0.13 in ./.conda/lib/python3.11/site-packages (from faster-whisper==1.0.0->whisperx==3.1.1) (0.15.2)\n",
      "Requirement already satisfied: onnxruntime<2,>=1.14 in ./.conda/lib/python3.11/site-packages (from faster-whisper==1.0.0->whisperx==3.1.1) (1.18.1)\n",
      "Requirement already satisfied: asteroid-filterbanks>=0.4 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (0.4.0)\n",
      "Requirement already satisfied: einops>=0.6.0 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (0.8.0)\n",
      "Requirement already satisfied: lightning>=2.0.1 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (2.3.3)\n",
      "Requirement already satisfied: omegaconf<3.0,>=2.1 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (2.3.0)\n",
      "Requirement already satisfied: pyannote.core>=5.0.0 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (5.0.0)\n",
      "Requirement already satisfied: pyannote.database>=5.0.1 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (5.1.0)\n",
      "Requirement already satisfied: pyannote.metrics>=3.2 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (3.2.1)\n",
      "Requirement already satisfied: pyannote.pipeline>=3.0.1 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (3.0.1)\n",
      "Requirement already satisfied: pytorch-metric-learning>=2.1.0 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (2.6.0)\n",
      "Requirement already satisfied: rich>=12.0.0 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (13.7.1)\n",
      "Requirement already satisfied: semver>=3.0.0 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (3.0.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (0.12.1)\n",
      "Requirement already satisfied: speechbrain>=0.5.14 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (1.0.0)\n",
      "Requirement already satisfied: tensorboardX>=2.6 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (2.6.2.2)\n",
      "Requirement already satisfied: torch-audiomentations>=0.11.0 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (0.11.1)\n",
      "Requirement already satisfied: torchmetrics>=0.11.0 in ./.conda/lib/python3.11/site-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (1.4.0.post0)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.11/site-packages (from torch>=2->whisperx==3.1.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.conda/lib/python3.11/site-packages (from torch>=2->whisperx==3.1.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.conda/lib/python3.11/site-packages (from torch>=2->whisperx==3.1.1) (1.12)\n",
      "Requirement already satisfied: networkx in ./.conda/lib/python3.11/site-packages (from torch>=2->whisperx==3.1.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.11/site-packages (from torch>=2->whisperx==3.1.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.conda/lib/python3.11/site-packages (from torch>=2->whisperx==3.1.1) (2024.6.1)\n",
      "Requirement already satisfied: click in ./.conda/lib/python3.11/site-packages (from nltk->whisperx==3.1.1) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.conda/lib/python3.11/site-packages (from nltk->whisperx==3.1.1) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.conda/lib/python3.11/site-packages (from nltk->whisperx==3.1.1) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in ./.conda/lib/python3.11/site-packages (from nltk->whisperx==3.1.1) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./.conda/lib/python3.11/site-packages (from pandas->whisperx==3.1.1) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.11/site-packages (from pandas->whisperx==3.1.1) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.11/site-packages (from pandas->whisperx==3.1.1) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/lib/python3.11/site-packages (from pandas->whisperx==3.1.1) (2023.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.11/site-packages (from transformers->whisperx==3.1.1) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.11/site-packages (from transformers->whisperx==3.1.1) (6.0.1)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (from transformers->whisperx==3.1.1) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.conda/lib/python3.11/site-packages (from transformers->whisperx==3.1.1) (0.4.3)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in ./.conda/lib/python3.11/site-packages (from lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (0.11.6)\n",
      "Requirement already satisfied: pytorch-lightning in ./.conda/lib/python3.11/site-packages (from lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (2.3.3)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in ./.conda/lib/python3.11/site-packages (from omegaconf<3.0,>=2.1->pyannote.audio==3.1.1->whisperx==3.1.1) (4.9.3)\n",
      "Requirement already satisfied: coloredlogs in ./.conda/lib/python3.11/site-packages (from onnxruntime<2,>=1.14->faster-whisper==1.0.0->whisperx==3.1.1) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.conda/lib/python3.11/site-packages (from onnxruntime<2,>=1.14->faster-whisper==1.0.0->whisperx==3.1.1) (24.3.25)\n",
      "Requirement already satisfied: protobuf in ./.conda/lib/python3.11/site-packages (from onnxruntime<2,>=1.14->faster-whisper==1.0.0->whisperx==3.1.1) (5.27.2)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.4 in ./.conda/lib/python3.11/site-packages (from pyannote.core>=5.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (2.4.0)\n",
      "Requirement already satisfied: scipy>=1.1 in ./.conda/lib/python3.11/site-packages (from pyannote.core>=5.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (1.14.0)\n",
      "Requirement already satisfied: typer>=0.12.1 in ./.conda/lib/python3.11/site-packages (from pyannote.database>=5.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (0.12.3)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in ./.conda/lib/python3.11/site-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (1.5.1)\n",
      "Requirement already satisfied: docopt>=0.6.2 in ./.conda/lib/python3.11/site-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (0.6.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in ./.conda/lib/python3.11/site-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (0.9.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in ./.conda/lib/python3.11/site-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (3.9.1)\n",
      "Requirement already satisfied: optuna>=3.1 in ./.conda/lib/python3.11/site-packages (from pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (3.6.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->whisperx==3.1.1) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.conda/lib/python3.11/site-packages (from rich>=12.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.conda/lib/python3.11/site-packages (from rich>=12.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (2.18.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./.conda/lib/python3.11/site-packages (from soundfile>=0.12.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.16.0)\n",
      "Requirement already satisfied: hyperpyyaml in ./.conda/lib/python3.11/site-packages (from speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1) (1.2.2)\n",
      "Requirement already satisfied: sentencepiece in ./.conda/lib/python3.11/site-packages (from speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1) (0.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.conda/lib/python3.11/site-packages (from sympy->torch>=2->whisperx==3.1.1) (1.3.0)\n",
      "Requirement already satisfied: julius<0.3,>=0.2.3 in ./.conda/lib/python3.11/site-packages (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.2.7)\n",
      "Requirement already satisfied: librosa>=0.6.0 in ./.conda/lib/python3.11/site-packages (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.10.2.post1)\n",
      "Requirement already satisfied: torch-pitch-shift>=1.2.2 in ./.conda/lib/python3.11/site-packages (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (1.2.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.11/site-packages (from jinja2->torch>=2->whisperx==3.1.1) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests->transformers->whisperx==3.1.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests->transformers->whisperx==3.1.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests->transformers->whisperx==3.1.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests->transformers->whisperx==3.1.1) (2024.7.4)\n",
      "Requirement already satisfied: pycparser in ./.conda/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio==3.1.1->whisperx==3.1.1) (2.22)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.conda/lib/python3.11/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (3.9.5)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./.conda/lib/python3.11/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (3.0.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./.conda/lib/python3.11/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./.conda/lib/python3.11/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.60.0)\n",
      "Requirement already satisfied: pooch>=1.1 in ./.conda/lib/python3.11/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./.conda/lib/python3.11/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.4.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in ./.conda/lib/python3.11/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./.conda/lib/python3.11/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (1.0.8)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.conda/lib/python3.11/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.conda/lib/python3.11/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.conda/lib/python3.11/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.conda/lib/python3.11/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in ./.conda/lib/python3.11/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.conda/lib/python3.11/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (3.1.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in ./.conda/lib/python3.11/site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.13.2)\n",
      "Requirement already satisfied: colorlog in ./.conda/lib/python3.11/site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (6.8.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in ./.conda/lib/python3.11/site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (2.0.31)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.conda/lib/python3.11/site-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (3.5.0)\n",
      "Requirement already satisfied: primePy>=1.3 in ./.conda/lib/python3.11/site-packages (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (1.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.conda/lib/python3.11/site-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.5.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.conda/lib/python3.11/site-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper==1.0.0->whisperx==3.1.1) (10.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.28 in ./.conda/lib/python3.11/site-packages (from hyperpyyaml->speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1) (0.18.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.9.4)\n",
      "Requirement already satisfied: Mako in ./.conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.3.5)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in ./.conda/lib/python3.11/site-packages (from numba>=0.51.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./.conda/lib/python3.11/site-packages (from pooch>=1.1->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (4.2.2)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in ./.conda/lib/python3.11/site-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1) (0.2.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "%pip install git+https://github.com/m-bain/whisperx.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-14.5-arm64-arm-64bit\n",
      "PyTorch Version: 2.4.0\n",
      "\n",
      "Python 3.11.9 (main, Apr 19 2024, 11:43:47) [Clang 14.0.6 ]\n",
      "NVIDIA/CUDA GPU is NOT AVAILABLE\n",
      "MPS (Apple Metal) is AVAILABLE\n",
      "Target device is mps\n"
     ]
    }
   ],
   "source": [
    "# What version of Python do you have?\n",
    "import sys\n",
    "import platform\n",
    "import torch\n",
    "import os\n",
    "import whisperx\n",
    "\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(\"NVIDIA/CUDA GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "print(f\"Target device is {device}\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"en\"\n",
    "model_size = \"small\"  # \"large-v3\" recommended for production\n",
    "batch_size = 16  # reduce if low on GPU mem\n",
    "compute_type = \"int8\"  # change to \"int8\" if low on GPU mem (may reduce accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.4.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "model = whisperx.load_model(\n",
    "    model_size, \"cpu\", language=language, compute_type=compute_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"./mp3_audio_files/audio.mp3\"\n",
    "audio = whisperx.load_audio(audio_path)\n",
    "result = model.transcribe(audio_path, batch_size=batch_size, language=language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2.16,  21.37, \"Retrieval augmented generation, or RAG, has become a key method for getting LMS to answer questions over a user's own data. But to actually build and productionize a high-quality RAG system, it helps a lot to have effective retrieval techniques to give the LMS highly relevant context to generate its answer.\"\n",
      "  21.37,  43.83, \"and also to have an effective evaluation framework to help you efficiently iterate and improve your RAG system both during initial development and during post deployment maintenance. This course covers two advanced retrieval methods, sentence window retrieval and auto-emerging retrieval that deliver a significantly better context to BLM than simpler methods.\"\n",
      "  43.83,  63.22, \"It also covers how to evaluate your LLM question-answering system with three evaluation metrics, contact relevance, droughtedness, and answer relevance. I'm excited to introduce Jerry Liu, co-founder and CO of Larmor and Text, and Andrew Plumdata, co-founder and chief scientist of TrueEra.\"\n",
      "  63.22,  90.27, \"For a long time, I've enjoyed following Jerry and Lamu index on social media and getting tips on evolving rat practices. So I'm looking forward to him teaching this body of knowledge more systematically here at a new pump has been a professor at CMU has done research for over a decade on trustworthy AI and how to monitor, evaluate and optimize AI app effectiveness. Thanks, Andrew. It's great to be here. Great to be with you, Andrew.\"\n",
      "  90.86, 116.92, \"Sentence window retrieval gives an LLN better context by retrieving not just the most relevant sentence, but the window of sentences that occur before and after it in the document. Auto-emerging retrieval organizes the document into a tree-like structure where each parent node's text is divided among its child nodes. When enough child nodes are identified as relevant to a user's question, then the entire text of the parent node is provided as context for the LLN.\"\n",
      " 116.92, 138.49, \"I know this sounds like a lot of steps, but don't worry, we'll go over it in detail on code later. But the main takeaway is that this provides a way to dynamically retrieve more coherent chunks of text than simpler methods. To evaluate RAG-based LLM apps, the RAG triad, a triad of metrics for the three main steps of a RAG's execution, is quite effective.\"\n",
      " 138.49, 159.22, \"For example, we'll cover in detail how to compute context relevance, which measures how relevant the retrieved chance of text are to the user's question. This helps you identify and debug possible issues with how your system is retrieving context for the LLM in the QA system.\"\n",
      " 159.38, 175.23, \"but that's only part of the overall QA system. We'll also cover additional evaluation metrics such as groundedness and answer relevance that let you systematically analyze what parts of your system are or are not yet working well.\"\n",
      " 175.56, 197.91, \"so that you can go in in a targeted way to improve whatever part needs the most work. If you're familiar with the concept of error analysis and machine learning, this has similarities. And I've found that taking this sort of systematic approach helps you be much more efficient in building a reliable QA system.\"\n",
      " 198.51, 226.44, \"The goal of this course is to help you build production-ready write-based LLM apps. An important part of getting production-ready is to iterate in a systematic way on the system. In the later half of this course, you gain hands-on practice iterating using these retrieval methods and evaluation methods. And you also see how to use systematic experiment tracking to establish a baseline and then quickly improve on that.\"\n",
      " 226.75, 253.56, \"We'll also share some suggestions for tuning these two retrieval methods, based on our experience assisting partners who are building RAG apps. Many people have worked to create this course. I'd like to thank on the Lamindex side, Logan Machowicz, and on the Truero side, Shia Xen, Joshua Rainey, and Barbara Lewis, from D-Bloody.ai, Eddie Hsu, and Tiara Ezzedine also contributed to this course.\"\n",
      " 253.76, 278.00, \"The next lesson will give you an overview of what you'll see in the rest of the course. You'll try out question answering systems that use sentence window retrieval or auto merging retrieval and compare their performance on the rag triad. Contacts relevance, groundedness, and answer relevance. Sounds great. Let's get started. And I think you'd be really clean up with this rag stuff. Laughed on it.\"\n",
      "   2.16,  21.37, \"Retrieval augmented generation, or RAG, has become a key method for getting LMS to answer questions over a user's own data. But to actually build and productionize a high-quality RAG system, it helps a lot to have effective retrieval techniques to give the LMS highly relevant context to generate its answer.\"\n",
      "  21.37,  43.83, \"and also to have an effective evaluation framework to help you efficiently iterate and improve your RAG system both during initial development and during post deployment maintenance. This course covers two advanced retrieval methods, sentence window retrieval and auto-emerging retrieval that deliver a significantly better context to BLM than simpler methods.\"\n",
      "  43.83,  63.22, \"It also covers how to evaluate your LLM question-answering system with three evaluation metrics, contact relevance, droughtedness, and answer relevance. I'm excited to introduce Jerry Liu, co-founder and CO of Larmor and Text, and Andrew Plumdata, co-founder and chief scientist of TrueEra.\"\n",
      "  63.22,  90.27, \"For a long time, I've enjoyed following Jerry and Lamu index on social media and getting tips on evolving rat practices. So I'm looking forward to him teaching this body of knowledge more systematically here at a new pump has been a professor at CMU has done research for over a decade on trustworthy AI and how to monitor, evaluate and optimize AI app effectiveness. Thanks, Andrew. It's great to be here. Great to be with you, Andrew.\"\n",
      "  90.86, 116.92, \"Sentence window retrieval gives an LLN better context by retrieving not just the most relevant sentence, but the window of sentences that occur before and after it in the document. Auto-emerging retrieval organizes the document into a tree-like structure where each parent node's text is divided among its child nodes. When enough child nodes are identified as relevant to a user's question, then the entire text of the parent node is provided as context for the LLN.\"\n",
      " 116.92, 138.49, \"I know this sounds like a lot of steps, but don't worry, we'll go over it in detail on code later. But the main takeaway is that this provides a way to dynamically retrieve more coherent chunks of text than simpler methods. To evaluate RAG-based LLM apps, the RAG triad, a triad of metrics for the three main steps of a RAG's execution, is quite effective.\"\n",
      " 138.49, 159.22, \"For example, we'll cover in detail how to compute context relevance, which measures how relevant the retrieved chance of text are to the user's question. This helps you identify and debug possible issues with how your system is retrieving context for the LLM in the QA system.\"\n",
      " 159.38, 175.23, \"but that's only part of the overall QA system. We'll also cover additional evaluation metrics such as groundedness and answer relevance that let you systematically analyze what parts of your system are or are not yet working well.\"\n",
      " 175.56, 197.91, \"so that you can go in in a targeted way to improve whatever part needs the most work. If you're familiar with the concept of error analysis and machine learning, this has similarities. And I've found that taking this sort of systematic approach helps you be much more efficient in building a reliable QA system.\"\n",
      " 198.51, 226.44, \"The goal of this course is to help you build production-ready write-based LLM apps. An important part of getting production-ready is to iterate in a systematic way on the system. In the later half of this course, you gain hands-on practice iterating using these retrieval methods and evaluation methods. And you also see how to use systematic experiment tracking to establish a baseline and then quickly improve on that.\"\n",
      " 226.75, 253.56, \"We'll also share some suggestions for tuning these two retrieval methods, based on our experience assisting partners who are building RAG apps. Many people have worked to create this course. I'd like to thank on the Lamindex side, Logan Machowicz, and on the Truero side, Shia Xen, Joshua Rainey, and Barbara Lewis, from D-Bloody.ai, Eddie Hsu, and Tiara Ezzedine also contributed to this course.\"\n",
      " 253.76, 278.00, \"The next lesson will give you an overview of what you'll see in the rest of the course. You'll try out question answering systems that use sentence window retrieval or auto merging retrieval and compare their performance on the rag triad. Contacts relevance, groundedness, and answer relevance. Sounds great. Let's get started. And I think you'd be really clean up with this rag stuff. Laughed on it.\"\n"
     ]
    }
   ],
   "source": [
    "max_segments = 30  # max number of segments to print\n",
    "for segment in (\n",
    "    result[\"segments\"][: max_segments // 2] + result[\"segments\"][-max_segments // 2 :]\n",
    "):\n",
    "    print(\n",
    "        f\"{segment['start']:7.2f},{segment['end']:7.2f}, \\\"{segment['text'].strip()}\\\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_language = result[\"language\"]\n",
    "model_a, metadata = whisperx.load_align_model(\n",
    "    language_code=align_language, device=device\n",
    ")\n",
    "result = whisperx.align(\n",
    "    result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False\n",
    ")\n",
    "result[\"language\"] = align_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2.22,   9.56, \"Retrieval augmented generation, or RAG, has become a key method for getting LMS to answer questions over a user's own data.\"\n",
      "  10.30,  20.81, \"But to actually build and productionize a high-quality RAG system, it helps a lot to have effective retrieval techniques to give the LMS highly relevant context to generate its answer.\"\n",
      "  21.51,  32.30, \"and also to have an effective evaluation framework to help you efficiently iterate and improve your RAG system both during initial development and during post deployment maintenance.\"\n",
      "  32.64,  43.27, \"This course covers two advanced retrieval methods, sentence window retrieval and auto-emerging retrieval that deliver a significantly better context to BLM than simpler methods.\"\n",
      "  43.97,  53.23, \"It also covers how to evaluate your LLM question-answering system with three evaluation metrics, contact relevance, droughtedness, and answer relevance.\"\n",
      "  54.27,  62.42, \"I'm excited to introduce Jerry Liu, co-founder and CO of Larmor and Text, and Andrew Plumdata, co-founder and chief scientist of TrueEra.\"\n",
      "  63.32,  70.48, \"For a long time, I've enjoyed following Jerry and Lamu index on social media and getting tips on evolving rat practices.\"\n",
      "  71.02,  86.24, \"So I'm looking forward to him teaching this body of knowledge more systematically here at a new pump has been a professor at CMU has done research for over a decade on trustworthy AI and how to monitor, evaluate and optimize AI app effectiveness.\"\n",
      "  86.96,  87.56, \"Thanks, Andrew.\"\n",
      "  87.62,  88.34, \"It's great to be here.\"\n",
      "  89.11,  90.14, \"Great to be with you, Andrew.\"\n",
      "  90.96, 100.31, \"Sentence window retrieval gives an LLN better context by retrieving not just the most relevant sentence, but the window of sentences that occur before and after it in the document.\"\n",
      " 101.47, 108.47, \"Auto-emerging retrieval organizes the document into a tree-like structure where each parent node's text is divided among its child nodes.\"\n",
      " 109.03, 116.04, \"When enough child nodes are identified as relevant to a user's question, then the entire text of the parent node is provided as context for the LLN.\"\n",
      " 117.06, 121.30, \"I know this sounds like a lot of steps, but don't worry, we'll go over it in detail on code later.\"\n",
      " 188.04, 197.79, \"And I've found that taking this sort of systematic approach helps you be much more efficient in building a reliable QA system.\"\n",
      " 198.65, 203.65, \"The goal of this course is to help you build production-ready write-based LLM apps.\"\n",
      " 204.59, 210.01, \"An important part of getting production-ready is to iterate in a systematic way on the system.\"\n",
      " 210.85, 218.24, \"In the later half of this course, you gain hands-on practice iterating using these retrieval methods and evaluation methods.\"\n",
      " 218.84, 226.20, \"And you also see how to use systematic experiment tracking to establish a baseline and then quickly improve on that.\"\n",
      " 226.79, 233.83, \"We'll also share some suggestions for tuning these two retrieval methods, based on our experience assisting partners who are building RAG apps.\"\n",
      " 234.55, 236.85, \"Many people have worked to create this course.\"\n",
      " 237.41, 253.38, \"I'd like to thank on the Lamindex side, Logan Machowicz, and on the Truero side, Shia Xen, Joshua Rainey, and Barbara Lewis, from D-Bloody.ai, Eddie Hsu, and Tiara Ezzedine also contributed to this course.\"\n",
      " 253.80, 256.94, \"The next lesson will give you an overview of what you'll see in the rest of the course.\"\n",
      " 257.31, 264.59, \"You'll try out question answering systems that use sentence window retrieval or auto merging retrieval and compare their performance on the rag triad.\"\n",
      " 265.09, 267.75, \"Contacts relevance, groundedness, and answer relevance.\"\n",
      " 268.51, 268.97, \"Sounds great.\"\n",
      " 269.31, 270.11, \"Let's get started.\"\n",
      " 270.27, 273.79, \"And I think you'd be really clean up with this rag stuff.\"\n",
      " 275.25, 275.79, \"Laughed on it.\"\n"
     ]
    }
   ],
   "source": [
    "max_segments = 30  # max number of segments to print\n",
    "for segment in (\n",
    "    result[\"segments\"][: max_segments // 2] + result[\"segments\"][-max_segments // 2 :]\n",
    "):\n",
    "    print(\n",
    "        f\"{segment['start']:7.2f},{segment['end']:7.2f}, \\\"{segment['text'].strip()}\\\"\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
