{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building ReAct agents with custom tools and Ollama 3 Instruct\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    "1. **Set Up the Environment**: Prepare your development environment with the necessary tools and libraries.\n",
    "2. **Define Custom Tools**: Create custom functions that can be used by the agent to perform specific tasks.\n",
    "3. **Integrate with Ollama Model**: Use the Ollama 3 Instruct model to generate responses based on user queries and tool descriptions.\n",
    "4. **Build a ReAct Agent**: Assemble the defined tools and the language model into a ReAct (Reasoning and Action) agent. This agent will not only process and respond to user queries but will also engage in dynamic reasoning to create, maintain, and adjust action plans. You'll learn how to implement the agent's ability to handle multi-step queries, integrate with external systems, and dynamically adapt its actions based on the context and real-time inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts\n",
    "\n",
    "### What are ReAct Agents?\n",
    "\n",
    "**ReAct (Reasoning and Action)** agents are a type of AI system that combines reasoning capabilities with the ability to take actions. Unlike traditional models that only generate responses based on input data, ReAct agents can use external tools and resources to gather additional information, perform tasks, and make decisions. This dynamic interplay between reasoning and action enables ReAct agents to handle complex, multi-step queries more effectively.\n",
    "\n",
    "ReAct agents operate by generating reasoning traces (thought processes) and then deciding on actions based on these traces. This allows them to interact with external systems, update their knowledge, and adapt their strategies in real-time.\n",
    "\n",
    "The basic components of a ReAct agent include:\n",
    "- 1. A large language model (LLM) that processes input queries and generates reasoning traces.\n",
    "- 2. A set of tools or APIs that the agent can use to perform specific tasks.\n",
    "- 3. A framework that integrates the LLM and tools, allowing the agent to reason and act dynamically.\n",
    "\n",
    "[ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "Before we dive into building our agent, we need to set up the necessary environment. This involves installing required packages and ensuring our Python environment is ready for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: termcolor in ./.conda/lib/python3.11/site-packages (2.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install termcolor for colored terminal outputs\n",
    "%pip install termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from termcolor import colored\n",
    "import operator\n",
    "import json\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the System Prompt and Model Configuration\n",
    "\n",
    "In this section, we define the system prompt and set up the configuration for the Ollama model. The system prompt provides context and instructions for the model, ensuring it understands how to use the available tools and respond to queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompt Template\n",
    "\n",
    "The system prompt is a crucial component in guiding the behavior of the language model. It provides a structured way to present the available tools and define how the model should respond to user queries.\n",
    "\n",
    "*Based on:*\n",
    "[Model Cards & Prompt formats Llama 3](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_system_prompt_template = \"\"\"\n",
    "You are designed to help with a variety of tasks, from answering questions \\\n",
    "    to providing summaries to other types of analyses.\n",
    "\n",
    "## Tools\n",
    "You have access to a wide variety of tools. You are responsible for using\n",
    "the tools in any sequence you deem appropriate to complete the task at hand.\n",
    "This may require breaking the task into subtasks and using different tools\n",
    "to complete each subtask.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tool_descriptions}\n",
    "\n",
    "## Output Format\n",
    "To answer the question, please use the following format.\n",
    "\n",
    "```\n",
    "Thought: I need to use a tool to help me answer the question.\n",
    "Action: tool name (one of the available) if using a tool.\n",
    "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n",
    "```\n",
    "\n",
    "Please ALWAYS start with a Thought.\n",
    "\n",
    "Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n",
    "\n",
    "If this format is used, the user will respond in the following format:\n",
    "\n",
    "```\n",
    "Observation: tool response\n",
    "```\n",
    "\n",
    "You should keep repeating the above format until you have enough information\n",
    "to answer the question without using any more tools. At that point, you MUST respond\n",
    "in the one of the following two formats:\n",
    "\n",
    "```\n",
    "Thought: I can answer without using any more tools.\n",
    "Answer: [your answer here]\n",
    "```\n",
    "\n",
    "```\n",
    "Thought: I cannot answer the question with the provided tools.\n",
    "Answer: Sorry, I cannot answer your query.\n",
    "```\n",
    "\n",
    "## Additional Rules\n",
    "- The answer MUST contain a sequence of bullet points that explain how you arrived at the answer. This can include aspects of the previous conversation history.\n",
    "- You MUST obey the function signature of each tool. Do NOT pass in no arguments if the function expects arguments.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Ollama Model\n",
    "\n",
    "The `setup_ollama_model` function configures the model settings, including the endpoint, model name, system prompt, and other parameters. This setup is essential for initializing the model with the correct configuration, ensuring it can process queries and utilize the tools effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ollama_model(model, system_prompt, temperature=0, stop=None):\n",
    "    \"\"\"\n",
    "    Sets up the Ollama model configuration.\n",
    "\n",
    "    Parameters:\n",
    "    model (str): The name of the model to use.\n",
    "    system_prompt (str): The system prompt to use.\n",
    "    temperature (float): The temperature setting for the model.\n",
    "    stop (str): The stop token for the model.\n",
    "\n",
    "    Returns:\n",
    "    dict: Configuration for the Ollama model.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model_endpoint\": \"http://localhost:11434/api/generate\",\n",
    "        \"model\": model,\n",
    "        \"system_prompt\": system_prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"stop\": stop,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example configuration\n",
    "ollama_config = setup_ollama_model(\n",
    "    model=\"llama3:instruct\", system_prompt=agent_system_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating custom tools\n",
    "\n",
    "This section introduces custom tools that our agent will use to perform specific tasks. These tools are functions designed to handle particular operations, such as mathematical calculations and string manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Calculator\n",
    "\n",
    "The `basic_calculator` function is designed to perform arithmetic operations on two numbers based on the input provided. It accepts a JSON string containing the numbers and the operation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_calculator(input_str):\n",
    "    \"\"\"\n",
    "    Perform a numeric operation on two numbers based on the input string.\n",
    "\n",
    "    Parameters:\n",
    "    'num1' (int): The first number.\n",
    "    'num2' (int): The second number.\n",
    "    'operation' (str): The operation to perform. Supported operations are 'add', 'subtract',\n",
    "                        'multiply', 'divide', 'floor_divide', 'modulus', 'power', 'lt',\n",
    "                        'le', 'eq', 'ne', 'ge', 'gt'.\n",
    "\n",
    "    Returns:\n",
    "    str: The formatted result of the operation.\n",
    "\n",
    "    Raises:\n",
    "    Exception: If an error occurs during the operation (e.g., division by zero).\n",
    "    ValueError: If an unsupported operation is requested or input is invalid.\n",
    "    \"\"\"\n",
    "    # Clean and parse the input string\n",
    "    try:\n",
    "        # Replace single quotes with double quotes\n",
    "        # input_str_clean = input_str.replace(\"'\", '\"')\n",
    "        # Remove any extraneous characters such as trailing quotes\n",
    "        # input_str_clean = input_str_clean.strip().strip('\"')\n",
    "\n",
    "        num1 = input_str.get(\"num1\")\n",
    "        num2 = input_str.get(\"num2\")\n",
    "        operation = input_str.get(\"operation\")\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        return str(e), \"Invalid input format. Please provide a valid JSON string.\"\n",
    "\n",
    "    # Define the supported operations\n",
    "    operations = {\n",
    "        \"add\": operator.add,\n",
    "        \"subtract\": operator.sub,\n",
    "        \"multiply\": operator.mul,\n",
    "        \"divide\": operator.truediv,\n",
    "        \"floor_divide\": operator.floordiv,\n",
    "        \"modulus\": operator.mod,\n",
    "        \"power\": operator.pow,\n",
    "        \"lt\": operator.lt,\n",
    "        \"le\": operator.le,\n",
    "        \"eq\": operator.eq,\n",
    "        \"ne\": operator.ne,\n",
    "        \"ge\": operator.ge,\n",
    "        \"gt\": operator.gt,\n",
    "    }\n",
    "\n",
    "    # Check if the operation is supported\n",
    "    if operation in operations:\n",
    "        try:\n",
    "            # Perform the operation\n",
    "            result = operations[operation](num1, num2)\n",
    "            result_formatted = (\n",
    "                f\"\\n\\nThe answer is: {result}.\\nCalculated with basic_calculator.\"\n",
    "            )\n",
    "            return result_formatted\n",
    "        except Exception as e:\n",
    "            return str(e), \"\\n\\nError during operation execution.\"\n",
    "    else:\n",
    "        return \"\\n\\nUnsupported operation. Please provide a valid operation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse String\n",
    "\n",
    "The `reverse_string` function takes a string as input and returns its reverse. This simple tool is useful for tasks involving string manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_string(input_string):\n",
    "    \"\"\"\n",
    "    Reverse the given string.\n",
    "\n",
    "    Parameters:\n",
    "    input_string (str): The string to be reversed.\n",
    "\n",
    "    Returns:\n",
    "    str: The reversed string.\n",
    "    \"\"\"\n",
    "    # Reverse the string using slicing\n",
    "    reversed_string = input_string[::-1]\n",
    "\n",
    "    reversed_string = f\"The reversed string is: {reversed_string}\\n\\nExecuted using the reverse_string function.\"\n",
    "    # print (f\"DEBUG: reversed_string: {reversed_string}\")\n",
    "    return reversed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use these tools within our agent, we register them in a list. This list will be referenced by the agent to determine which tools are available for use.\n",
    "tools = [basic_calculator, reverse_string]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Agent Class\n",
    "\n",
    "This section focuses on constructing the `Agent` class, which integrates the tools and the Ollama model. The class handles user queries, determines the appropriate tools to use, and executes the necessary actions. This structure encapsulates the core functionalities of the agent, making it a reusable and maintainable component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class Definition\n",
    "\n",
    "The `Agent` class is designed to manage the interaction between user queries, the available tools, and the language model. It includes methods for initializing the agent, preparing tool descriptions, generating responses, and executing tools.\n",
    "\n",
    "- The agent is initialized with a list of tools and a configuration dictionary for the model.\n",
    "- This includes setting up the model's endpoint, name, system prompt, temperature, headers, and stop token.\n",
    "- The `prepare_tools` method is called to set up the descriptions of the tools.\n",
    "\n",
    "**Preparing Tools**\n",
    "\n",
    "The `prepare_tools` method processes the tools' docstrings to extract and format descriptions and parameter information. This is crucial for generating a system prompt that informs the model of the available tools.\n",
    "\n",
    "**Generating Model Responses**\n",
    "\n",
    "The `think` method generates responses by sending user queries and the system prompt to the Ollama model.\n",
    "\n",
    "**Executing Tools**\n",
    "\n",
    "The `execute_tool` method determines which tool to use based on the model's response and executes it.\n",
    "\n",
    "**Main Method: Work**\n",
    "\n",
    "The `work` method is the primary entry point for processing user queries. It integrates the thinking and execution steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, tools, model_config):\n",
    "        \"\"\"\n",
    "        Initializes the agent with a list of tools and a model configuration.\n",
    "\n",
    "        Parameters:\n",
    "        tools (list): List of tool functions.\n",
    "        model_config (dict): Configuration for the model, including endpoint, model name, system prompt, etc.\n",
    "        \"\"\"\n",
    "        self.tools = tools\n",
    "        self.model_endpoint = model_config[\"model_endpoint\"]\n",
    "        self.model_name = model_config[\"model\"]\n",
    "        self.system_prompt = model_config[\"system_prompt\"]\n",
    "        self.temperature = model_config[\"temperature\"]\n",
    "        self.headers = model_config[\"headers\"]\n",
    "        self.stop = model_config[\"stop\"]\n",
    "        self.tool_descriptions = self.prepare_tools()\n",
    "        self.messages: list = []\n",
    "        if self.system_prompt:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": self.system_prompt})\n",
    "\n",
    "    def prepare_tools(self):\n",
    "        \"\"\"\n",
    "        Prepares descriptions of the available tools.\n",
    "\n",
    "        Returns:\n",
    "        str: JSON-like formatted string describing the tools.\n",
    "        \"\"\"\n",
    "        tools_info = []\n",
    "\n",
    "        for tool in self.tools:\n",
    "            name = tool.__name__\n",
    "            docstring = (\n",
    "                tool.__doc__.strip() if tool.__doc__ else \"No description available.\"\n",
    "            )\n",
    "            parts = docstring.split(\"\\n\\n\", 1)\n",
    "            description = parts[0].split(\"\\n\")[0]\n",
    "            parameters_section = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "            param_dict = {}\n",
    "\n",
    "            if \"Parameters:\" in parameters_section:\n",
    "                parameters_lines = parameters_section.split(\"\\n\")\n",
    "                param_section_started = False\n",
    "\n",
    "                for line in parameters_lines:\n",
    "                    if \"Parameters:\" in line:\n",
    "                        param_section_started = True\n",
    "                        continue\n",
    "                    if not param_section_started or not line.strip():\n",
    "                        continue\n",
    "\n",
    "                    if \": \" in line:\n",
    "                        param_name_type_desc = line.split(\":\", 1)\n",
    "                        if len(param_name_type_desc) == 2:\n",
    "                            param_name_type, description = param_name_type_desc\n",
    "                            param_name = param_name_type.split(\"(\", 1)[0].strip()\n",
    "                            param_type = (\n",
    "                                param_name_type.split(\"(\", 1)[1].split(\")\", 1)[0]\n",
    "                                if \"(\" in param_name_type\n",
    "                                else \"str\"\n",
    "                            )\n",
    "                            param_dict[param_name] = {\n",
    "                                \"param_type\": param_type.strip(),\n",
    "                                \"description\": description.strip(),\n",
    "                                \"required\": True,\n",
    "                            }\n",
    "\n",
    "            tool_info = {\n",
    "                \"name\": name,\n",
    "                \"description\": description,\n",
    "                \"parameters\": param_dict,\n",
    "            }\n",
    "            tools_info.append(tool_info)\n",
    "\n",
    "        tools_description = \"\\n\".join(\n",
    "            [\n",
    "                f\"Use the function '{info['name']}' to: {info['description']}\\n{json.dumps(info, indent=2)}\"\n",
    "                for info in tools_info\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return tools_description\n",
    "\n",
    "    def think(self, prompt):\n",
    "        \"\"\"\n",
    "        Generates a response from the model based on the provided prompt.\n",
    "\n",
    "        Parameters:\n",
    "        prompt (str): The user query.\n",
    "\n",
    "        Returns:\n",
    "        dict: The response from the model.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"format\": \"json\",\n",
    "            \"prompt\": prompt,\n",
    "            \"system\": self.system_prompt.format(\n",
    "                tool_descriptions=self.tool_descriptions\n",
    "            ),\n",
    "            \"stream\": False,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"stop\": self.stop,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.model_endpoint, headers=self.headers, data=json.dumps(payload)\n",
    "            )\n",
    "            response_json = response.json()\n",
    "            response_dict = json.loads(response_json[\"response\"])\n",
    "\n",
    "            # print(f\"\\n\\n{response_dict}\")\n",
    "            self.messages.append({\"role\": \"assistant\", \"content\": response_dict})\n",
    "            return response_dict\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error in invoking model! {str(e)}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def work(self, prompt):\n",
    "        \"\"\"\n",
    "        The main method to process a user query, generate a response, and execute the appropriate tool.\n",
    "\n",
    "        Parameters:\n",
    "        prompt (str): The user query.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        result = self.think(prompt)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "agent = Agent(tools=tools, model_config=ollama_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agent\n",
    "\n",
    "This section demonstrates how to run the agent with example prompts. We will use the `work` method of the `Agent` class to process different types of user queries, showcasing the agent's ability to select and execute the appropriate tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Thought': 'I need to use a tool to help me answer the question.'}\n",
      "{'Thought': 'I need to use a tool to help me answer the question.'}\n",
      "{'Thought: I need to use a tool to help me answer the question.': 'Action: basic_calculator'}\n",
      "{'Thought': 'I need to use a tool to help me answer this question.'}\n",
      "{'Thought': 'I need to use a basic calculator to find the result.', 'Action': 'basic_calculator', 'Action Input': {'num1': 20, 'num2': 20, 'operation': 'add'}}\n",
      "\u001b[36m\n",
      "\n",
      "The answer is: 40.\n",
      "Calculated with basic_calculator.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def loop(max_iterations=10, query: str = \"\"):\n",
    "\n",
    "    agent = Agent(tools=tools, model_config=ollama_config)\n",
    "\n",
    "    next_prompt = query\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < max_iterations:\n",
    "        i += 1\n",
    "        result = agent.work(next_prompt)\n",
    "        print(result)\n",
    "\n",
    "        if \"Action\" in result:\n",
    "            tool_choice = result[\"Action\"]\n",
    "            arg = result[\"Action Input\"]\n",
    "\n",
    "            for tool in tools:\n",
    "                if tool.__name__ == tool_choice:\n",
    "                    try:\n",
    "                        result = tool(arg)\n",
    "                        next_prompt = f\"Observation: {result}\"\n",
    "                        print(colored(result, \"cyan\"))\n",
    "                    except Exception as e:\n",
    "                        print(\n",
    "                            colored(f\"Error executing tool {tool_choice}: {str(e)}\", \"red\")\n",
    "                        )\n",
    "                        next_prompt = f\"Error executing tool {tool_choice}: {str(e)}\"\n",
    "                    return\n",
    "            else:\n",
    "                next_prompt = f\"Tool {tool_choice} not found or unsupported operation.\"\n",
    "                print(colored(f\"Tool {tool_choice} not found or unsupported operation.\", \"red\"))\n",
    "\n",
    "loop(query=\"What is 20+20?\")\n",
    "# print(agent.messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
