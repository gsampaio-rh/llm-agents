{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LLM Agents with LLama3 and LangGraph\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, you will learn how to build and deploy LLM (Large Language Model) agents using LLama3 and LangGraph. These agents will be capable of processing natural language inputs, generating comprehensive plans, and handling complex workflows. By the end of this notebook, you will understand how to:\n",
    "\n",
    "1. Set up and configure a language model using LLama3.\n",
    "2. Define and manage the state for agents in a workflow.\n",
    "3. Implement and customize agent classes for specific tasks.\n",
    "4. Construct and compile a workflow graph using LangGraph.\n",
    "5. Execute the workflow and handle outputs effectively.\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "Before diving into the implementation, it's essential to understand some basic concepts:\n",
    "\n",
    "- **LLM (Large Language Model):** A machine learning model trained on vast amounts of text data to understand and generate human-like language. LLama3 is an example of such a model.\n",
    "\n",
    "- **Agent:** A software component that interacts with the LLM to perform specific tasks, such as generating responses or processing information. In this notebook, we implement agents that can handle various aspects of a workflow.\n",
    "\n",
    "- **State:** A shared data structure that stores the context and data required by agents. The state is critical for maintaining continuity and passing information between different parts of the workflow.\n",
    "\n",
    "- **Workflow Graph:** A structured representation of the workflow, where nodes represent agents and edges define the flow of information and control. LangGraph is used to construct and manage these workflow graphs.\n",
    "\n",
    "- **Prompt Engineering:** The process of crafting prompts that guide the LLM's responses. Proper prompt engineering is crucial for ensuring the model generates relevant and accurate outputs.\n",
    "\n",
    "Understanding these concepts will provide a solid foundation as we proceed with the practical implementation of LLM agents and workflows in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "Before we dive into building our agent, we need to set up the necessary environment. This involves installing required packages and ensuring our Python environment is ready for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install termcolor for colored terminal outputs\n",
    "%pip install termcolor langgraph pyaudio faster-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from termcolor import colored\n",
    "import json\n",
    "import requests\n",
    "from faster_whisper import WhisperModel\n",
    "import queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Configuration\n",
    "\n",
    "In this section, we set up the configuration for the Ollama model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Ollama Model\n",
    "\n",
    "The `setup_ollama_model` function configures the model settings, including the endpoint, model name, system prompt, and other parameters. This setup is essential for initializing the model with the correct configuration, ensuring it can process queries and utilize the tools effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ollama_model(model, temperature=0, stop=None):\n",
    "    \"\"\"\n",
    "    Sets up the Ollama model configuration.\n",
    "\n",
    "    Parameters:\n",
    "    model (str): The name of the model to use.\n",
    "    temperature (float): The temperature setting for the model.\n",
    "    stop (str): The stop token for the model.\n",
    "\n",
    "    Returns:\n",
    "    dict: Configuration for the Ollama model.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model_endpoint\": \"http://localhost:11434/api/generate\",\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"stop\": stop,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example configuration\n",
    "ollama_config = setup_ollama_model(model=\"llama3:instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "\n",
    "# MODEL_TYPE: Defines the type of Whisper model to use. Options include \"small\", \"medium\", \"large\", etc.\n",
    "# Smaller models are faster but less accurate, while larger models are more accurate but require more resources.\n",
    "MODEL_TYPE = \"small\"\n",
    "\n",
    "# RUN_TYPE: Specifies whether the model should run on a CPU or GPU. Set to \"gpu\" for GPU acceleration if available.\n",
    "RUN_TYPE = \"cpu\"  # Change to \"gpu\" if you have a GPU available\n",
    "\n",
    "# For CPU usage:\n",
    "# NUM_WORKERS: Number of worker threads used by the model for CPU operations. More workers can speed up processing.\n",
    "NUM_WORKERS = 10\n",
    "\n",
    "# CPU_THREADS: Number of threads to use for CPU operations. This should ideally match the number of CPU cores available.\n",
    "CPU_THREADS = 4\n",
    "\n",
    "# For GPU usage:\n",
    "# GPU_DEVICE_INDICES: List of GPU indices to use. For example, [0, 1] will use the first two GPUs.\n",
    "GPU_DEVICE_INDICES = [0, 1, 2, 3]\n",
    "\n",
    "# VAD_FILTER: Voice Activity Detection filter flag. When True, the model will filter out non-speech audio segments.\n",
    "VAD_FILTER = True\n",
    "\n",
    "# Visualization (expected max number of characters for LENGTH_IN_SEC audio)\n",
    "# MAX_SENTENCE_CHARACTERS: The maximum number of characters expected in a single line of transcription.\n",
    "# This helps in formatting the display of transcribed text.\n",
    "MAX_SENTENCE_CHARACTERS = 80\n",
    "\n",
    "# Audio settings\n",
    "\n",
    "# STEP_IN_SEC: The length of each audio chunk in seconds. This defines the duration of audio data captured in one go.\n",
    "STEP_IN_SEC: int = 1\n",
    "\n",
    "# LENGTH_IN_SEC: Maximum duration of audio data to process at once. This sets the maximum length of audio data that will be processed together.\n",
    "LENGTH_IN_SEC: int = 6\n",
    "\n",
    "# NB_CHANNELS: The number of audio channels. 1 for mono, 2 for stereo.\n",
    "NB_CHANNELS = 1\n",
    "\n",
    "# RATE: The sample rate of the audio data (in Hz). Common rates include 16000 (16kHz) and 44100 (44.1kHz).\n",
    "RATE = 16000\n",
    "\n",
    "# CHUNK: The number of audio samples per frame. This typically matches the sample rate for 1 second of audio data.\n",
    "CHUNK = RATE\n",
    "\n",
    "## INPUT_DEVICE_ID\n",
    "INPUT_DEVICE_ID = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the Whisper model\n",
    "def create_whisper_model() -> WhisperModel:\n",
    "    if RUN_TYPE.lower() == \"gpu\":\n",
    "        whisper = WhisperModel(\n",
    "            MODEL_TYPE,\n",
    "            device=\"cuda\",\n",
    "            compute_type=\"float16\",\n",
    "            device_index=GPU_DEVICE_INDICES,\n",
    "            download_root=\"./models\",\n",
    "        )\n",
    "    elif RUN_TYPE.lower() == \"cpu\":\n",
    "        whisper = WhisperModel(\n",
    "            MODEL_TYPE,\n",
    "            device=\"cpu\",\n",
    "            compute_type=\"int8\",\n",
    "            num_workers=NUM_WORKERS,\n",
    "            cpu_threads=CPU_THREADS,\n",
    "            download_root=\"./models\",\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model type: {RUN_TYPE}\")\n",
    "\n",
    "    print(\"Loaded model\")\n",
    "    return whisper\n",
    "\n",
    "# Load the model\n",
    "print(\"Whisper model is ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Defining the Agent Graph State\n",
    "\n",
    "In this step, we define the structure of the state that our agents will use to store and communicate information. This state acts as a shared memory that different components of the system can access and modify. We use the `TypedDict` from the `typing` module to define the expected structure and types of data within the state. This helps ensure consistency and correctness when accessing or updating the state, making it easier to manage complex workflows and data dependencies.\n",
    "\n",
    "The `AgentGraphState` class includes fields for the research question, responses from the planner agent, and any final outputs or end states. The `get_agent_graph`_state function is used to retrieve specific parts of the state based on a key, facilitating modular and reusable access to the state data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Reducer for queues\n",
    "def queue_reducer(existing_queue: queue.Queue, new_data) -> queue.Queue:\n",
    "    if isinstance(new_data, list):\n",
    "        for item in new_data:\n",
    "            existing_queue.put(item)\n",
    "    else:\n",
    "        existing_queue.put(new_data)\n",
    "    return existing_queue\n",
    "\n",
    "\n",
    "# Define the state object for the agent graph\n",
    "class AgentGraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    This class defines the structure of the agent graph state.\n",
    "    \n",
    "    Attributes:\n",
    "    research_question (str): The main research question the agent is working on.\n",
    "    planner_response (list): A list to store responses from the planner agent.\n",
    "    end_chain (list): A list to store the final outputs or end states.\n",
    "    \"\"\"\n",
    "    start_chain: Annotated[list, add_messages]\n",
    "    audio_queue: Annotated[queue.Queue, queue_reducer]\n",
    "    length_queue: Annotated[queue.Queue, queue_reducer]\n",
    "    recording: bool\n",
    "    recording_response: Annotated[list, add_messages]\n",
    "    transcription_response: Annotated[list, add_messages]\n",
    "    end_chain: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# Function to retrieve specific parts of the agent state\n",
    "def get_agent_graph_state(state: AgentGraphState, state_key: str):\n",
    "    \"\"\"\n",
    "    Retrieves specific parts of the agent state based on the provided key.\n",
    "    \n",
    "    Parameters:\n",
    "    state (AgentGraphState): The current state of the agent.\n",
    "    state_key (str): The key indicating which part of the state to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "    list or None: The requested state data or None if the key is not recognized.\n",
    "    \"\"\"\n",
    "    if state_key == \"transcription_all\":\n",
    "        return state[\"transcription_response\"]\n",
    "    elif state_key == \"transcription_latest\":\n",
    "        return state[\"transcription_response\"][-1] if state[\"transcription_response\"] else []\n",
    "\n",
    "    if state_key == \"audio_queue\":\n",
    "        return state[\"audio_queue\"]\n",
    "    elif state_key == \"audio_latest\":\n",
    "        return state[\"audio_queue\"][-1] if state[\"audio_queue\"] else []\n",
    "\n",
    "    if state_key == \"length_queue\":\n",
    "        return state[\"length_queue\"]\n",
    "    elif state_key == \"length_latest\":\n",
    "        return state[\"length_queue\"][-1] if state[\"length_queue\"] else []\n",
    "\n",
    "    if state_key == \"recording_all\":\n",
    "        return state[\"recording_response\"]\n",
    "    elif state_key == \"recording_latest\":\n",
    "        return state[\"recording_response\"][-1] if state[\"recording_response\"] else []\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Initial state setup\n",
    "state = {\n",
    "    \"start_chain\": [],\n",
    "    \"recording\": False,\n",
    "    \"audio_queue\": queue.Queue(),\n",
    "    \"length_queue\": queue.Queue(maxsize=LENGTH_IN_SEC),\n",
    "    \"recording_response\": [],\n",
    "    \"transcription_response\": [],\n",
    "    \"end_chain\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agent Class Definition\n",
    "\n",
    "In this section, we define the `Agent` class, which serves as a base class for different types of agents in our system. An agent is a component that interacts with the language model to perform specific tasks, such as generating responses or processing information. The `Agent` class manages the configuration and state associated with the language model, allowing for easy setup and reuse of model configurations across different agents.\n",
    "\n",
    "The class includes methods for initializing the agent with a specific model configuration and updating the agent's state. The state encapsulates the context or memory of the agent, enabling it to maintain continuity across interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state: AgentGraphState, model_config: dict):\n",
    "        \"\"\"\n",
    "        Initializes the agent with a state and model configuration.\n",
    "\n",
    "        Parameters:\n",
    "        state (AgentGraphState): The initial state of the agent, containing necessary context and data.\n",
    "        model_config (dict): Configuration settings for the model, including endpoint, model name, temperature, etc.\n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        self.model_endpoint = model_config.get(\"model_endpoint\")\n",
    "        self.model_name = model_config.get(\"model\")\n",
    "        self.temperature = model_config.get(\n",
    "            \"temperature\", 0\n",
    "        )  # Default temperature is 0\n",
    "        self.headers = model_config.get(\"headers\", {\"Content-Type\": \"application/json\"})\n",
    "        self.stop = model_config.get(\"stop\")\n",
    "\n",
    "    def update_state(self, key: str, value: any):\n",
    "        \"\"\"\n",
    "        Updates the agent's state with a new key-value pair.\n",
    "\n",
    "        Parameters:\n",
    "        key (str): The key in the state dictionary to update.\n",
    "        value (any): The new value to associate with the specified key.\n",
    "        \"\"\"\n",
    "        # Print all keys in the state dictionary\n",
    "        \n",
    "        if key in self.state:\n",
    "            self.state[key] = value\n",
    "        else:\n",
    "            print(f\"Warning: Attempting to update a non-existing state key '{key}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Utility Functions\n",
    "\n",
    "Utility functions are auxiliary functions that assist with various common tasks within the notebook. They help keep the codebase clean and modular by encapsulating frequently used logic in separate functions. In this case, we have two utility functions: `check_for_content` and `get_current_utc_datetime`.\n",
    "\n",
    "- `check_for_content`: This function checks if a variable has a content attribute and returns its value if it exists. This is useful for handling different data types that may or may not have a content attribute.\n",
    "- `get_current_utc_datetime`: This function returns the current date and time in UTC format. This can be useful for timestamping events or logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "# Check if an attribute of the state dict has content\n",
    "def check_for_content(var):\n",
    "    \"\"\"\n",
    "    Checks if the provided variable has a 'content' attribute and returns it.\n",
    "\n",
    "    Parameters:\n",
    "    var (Any): The variable to check.\n",
    "\n",
    "    Returns:\n",
    "    Any: The 'content' attribute if it exists, otherwise the original variable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return var.content\n",
    "    except AttributeError:\n",
    "        return var\n",
    "\n",
    "\n",
    "# Get the current date and time in UTC\n",
    "def get_current_utc_datetime():\n",
    "    \"\"\"\n",
    "    Returns the current date and time in UTC.\n",
    "\n",
    "    Returns:\n",
    "    str: The current date and time in UTC, formatted as 'YYYY-MM-DD HH:MM:SS UTC'.\n",
    "    \"\"\"\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    return now_utc.strftime(\"%Y-%m-%d %H:%M:%S UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. AudioAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import os\n",
    "import wave\n",
    "import pyaudio\n",
    "import logging\n",
    "\n",
    "class AudioAgent(Agent):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state: AgentGraphState,\n",
    "        audio_folder: str = \"mp3_audio_files\",\n",
    "        listen_filename: str = \"tmp_listen\",\n",
    "    ):\n",
    "        super().__init__(\n",
    "            state, model_config={}\n",
    "        )\n",
    "        self.audio_folder = audio_folder\n",
    "        self.listen_filename = listen_filename\n",
    "        self.recording = False\n",
    "        self.record_lock = threading.Lock()\n",
    "\n",
    "    def listen(self) -> str:\n",
    "        thread = threading.Thread(target=self.record_audio)\n",
    "        input(\"Press ENTER to START recording...\")\n",
    "        with self.record_lock:\n",
    "            self.recording = True\n",
    "            self.update_state(\"recording\", True)\n",
    "        thread.start()\n",
    "        input(\"Press ENTER to STOP recording...\")\n",
    "        with self.record_lock:\n",
    "            self.recording = False\n",
    "            # self.update_state(\"recording\", False)\n",
    "        thread.join()\n",
    "\n",
    "    # Function to record audio from the microphone\n",
    "    def record_audio(self):\n",
    "        audio = pyaudio.PyAudio()\n",
    "        stream = audio.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            channels=NB_CHANNELS,\n",
    "            rate=RATE,\n",
    "            input=True,\n",
    "            frames_per_buffer=CHUNK,  # 1 second of audio\n",
    "            input_device_index=INPUT_DEVICE_ID,  # Specify the selected input device\n",
    "        )\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "        print(\"Microphone initialized, recording started...\")\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "        print(\"TRANSCRIPTION\")\n",
    "        print(\"-\" * 80)\n",
    "        audio_frames = []\n",
    "        try:\n",
    "            while self.recording:\n",
    "                audio_data = b\"\"\n",
    "                for _ in range(STEP_IN_SEC):\n",
    "                    chunk = stream.read(RATE)  # Read 1 second of audio data\n",
    "                    audio_data += chunk\n",
    "                    audio_frames.append(chunk)\n",
    "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    file_name = f\"{self.audio_folder}/{self.listen_filename}_{timestamp}.wav\"\n",
    "                    self.save_audio(audio, file_name, audio_frames)\n",
    "                    self.update_state(\n",
    "                        \"audio_queue\", audio_data\n",
    "                    )  # Put the 1-second audio data into the queue\n",
    "                    self.update_state(\"transcription_response\", file_name)\n",
    "                    print(colored(f\"Audio File Recorder ðŸŽ§: {file_name}\", \"cyan\"))\n",
    "\n",
    "        finally:\n",
    "            stream.stop_stream()\n",
    "            stream.close()\n",
    "            audio.terminate()\n",
    "            # self.update_state(\"recording\", False)\n",
    "            print(\"Microphone recording stopped.\")\n",
    "\n",
    "    def save_audio(self, audio, file_name, frames):\n",
    "        try:\n",
    "            with wave.open(file_name, \"wb\") as wf:\n",
    "                wf.setnchannels(NB_CHANNELS)\n",
    "                wf.setsampwidth(audio.get_sample_size(pyaudio.paInt16))\n",
    "                wf.setframerate(RATE)\n",
    "                wf.writeframes(b\"\".join(frames))\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed when trying to save audio: {e}\")\n",
    "\n",
    "    def invoke(\n",
    "        self,\n",
    "    ) -> dict:\n",
    "\n",
    "        try:\n",
    "            listen = self.listen()\n",
    "            return self.state\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error in invoking model! {str(e)}\")\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptAgent(Agent):\n",
    "\n",
    "    # Function to process audio and get transcription\n",
    "    def process_audio(audio_queue, length_queue):\n",
    "        while audio_queue.empty():\n",
    "            if length_queue.qsize() >= LENGHT_IN_SEC:\n",
    "                with length_queue.mutex:\n",
    "                    length_queue.queue.clear()\n",
    "                    print()\n",
    "\n",
    "            try:\n",
    "                audio_data = audio_queue.get(timeout=1)\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "\n",
    "            transcription_start_time = time.time()\n",
    "            length_queue.put(audio_data)\n",
    "\n",
    "            # Concatenate audio data in the length_queue\n",
    "            audio_data_to_process = b\"\"\n",
    "            for i in range(length_queue.qsize()):\n",
    "                # We index it so it won't get removed\n",
    "                audio_data_to_process += length_queue.queue[i]\n",
    "\n",
    "            try:\n",
    "                # Convert to NumPy array and normalize\n",
    "                audio_np = (\n",
    "                    np.frombuffer(audio_data_to_process, np.int16).astype(np.float32)\n",
    "                    / 255.0\n",
    "                )\n",
    "                transcription, language, language_probability = (\n",
    "                    execute_whisper_transcription(model, audio_np)\n",
    "                )\n",
    "                transcription = re.sub(r\"\\[.*\\]\", \"\", transcription)\n",
    "                transcription = re.sub(r\"\\(.*\\)\", \"\", transcription)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                transcription = \"Error\"\n",
    "\n",
    "            transcription_end_time = time.time()\n",
    "\n",
    "            # Display transcription\n",
    "            transcription_to_visualize = transcription.ljust(MAX_SENTENCE_CHARACTERS, \" \")\n",
    "            transcription_postprocessing_end_time = time.time()\n",
    "\n",
    "            sys.stdout.write(\"\\033[K\" + transcription_to_visualize + \"\\r\")\n",
    "\n",
    "            audio_queue.task_done()\n",
    "\n",
    "            # overall_elapsed_time = (\n",
    "            #     transcription_postprocessing_end_time - transcription_start_time\n",
    "            # )\n",
    "            # transcription_elapsed_time = transcription_end_time - transcription_start_time\n",
    "            # postprocessing_elapsed_time = (\n",
    "            #     transcription_postprocessing_end_time - transcription_end_time\n",
    "            # )\n",
    "            # stats[\"overall\"].append(overall_elapsed_time)\n",
    "            # stats[\"transcription\"].append(transcription_elapsed_time)\n",
    "            # stats[\"postprocessing\"].append(postprocessing_elapsed_time)\n",
    "\n",
    "        print(\"Audio processing stopped.\")\n",
    "\n",
    "    def invoke(self, audio_queue, length_queue) -> dict:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        print(colored(f\"Transcript ðŸ“: {timestamp}\", \"yellow\"))\n",
    "        self.update_state(\"recording\", True)\n",
    "        try:\n",
    "            # self.process_audio(audio_queue, length_queue)\n",
    "            # self.update_state(\"researcher_response\", response_formatted)\n",
    "            # print(colored(f\"Researcher ðŸ•µï¸â€â™‚ï¸: {response_formatted}\", \"yellow\"))\n",
    "            return self.state\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error in invoking model! {str(e)}\")\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. End Node\n",
    "\n",
    "The `EndNodeAgent` class is a specialized agent that marks the conclusion of the workflow in the agent graph. It extends the base `Agent` class and is primarily responsible for updating the state to indicate the end of the process. This agent is useful for workflows that require a clear termination point, ensuring that the system knows when all processing is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndNodeAgent(Agent):\n",
    "    def invoke(self) -> AgentGraphState:\n",
    "        \"\"\"\n",
    "        Marks the end of the workflow by updating the state.\n",
    "\n",
    "        This method updates the 'end_chain' key in the state to signify that\n",
    "        the workflow has reached its conclusion. It can be used to perform any\n",
    "        finalization tasks or simply to denote that the agent has completed its role.\n",
    "\n",
    "        Returns:\n",
    "        AgentGraphState: The updated state of the agent.\n",
    "        \"\"\"\n",
    "        self.update_state(\"end_chain\", \"end_chain\")\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartNodeAgent(Agent):\n",
    "    def invoke(self) -> AgentGraphState:\n",
    "        \"\"\"\n",
    "        Marks the end of the workflow by updating the state.\n",
    "\n",
    "        This method updates the 'end_chain' key in the state to signify that\n",
    "        the workflow has reached its conclusion. It can be used to perform any\n",
    "        finalization tasks or simply to denote that the agent has completed its role.\n",
    "\n",
    "        Returns:\n",
    "        AgentGraphState: The updated state of the agent.\n",
    "        \"\"\"\n",
    "        self.update_state(\"start_chain\", \"start_chain\")\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(data):\n",
    "    \"\"\"\n",
    "    Determines the next step in the workflow based on the agent's output.\n",
    "\n",
    "    Parameters:\n",
    "    data (dict): The data containing the agent's state.\n",
    "\n",
    "    Returns:\n",
    "    str: The next node to execute ('continue' for tool execution, 'end' to finish).\n",
    "    \"\"\"\n",
    "    # Check if the tools_response contains a final answer or indication to stop\n",
    "\n",
    "    recording = data[\"recording\"]\n",
    "\n",
    "    # Determine the next step based on the verification status\n",
    "    if recording:\n",
    "        print(colored(f\"Still recording...: \", \"green\"))\n",
    "        return \"transcript\"\n",
    "    else:\n",
    "        print(colored(f\"Recording has finished\", \"red\"))\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating and Compiling the Agent Graph\n",
    "\n",
    "In this section, we define the structure and flow of the agent-based system using the `StateGraph` class from the `langgraph` library. The graph consists of nodes, each representing a specific agent, and edges, which define the flow or sequence of operations. This setup enables the modeling of complex workflows where different agents can interact and pass information.\n",
    "\n",
    "- **`create_graph`:** This function initializes the `StateGraph` with a specific state structure (`AgentGraphState`). It then adds nodes for the `PlannerAgent` and `EndNodeAgent`, specifying the operations these agents should perform. The function sets the \"planner\" node as the entry point and the \"end\" node as the finish point, with an edge connecting them to define the workflow sequence.\n",
    "- **`compile_workflow`:** This function compiles the defined graph into a workflow that can be executed. The compiled workflow manages the execution of the nodes in the defined order, handling the flow of data and control through the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def create_graph() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Creates and configures the state graph for the agent workflow.\n",
    "\n",
    "    This function initializes the graph, adds the necessary nodes (agents), and\n",
    "    sets up the edges defining the flow of the workflow.\n",
    "\n",
    "    Returns:\n",
    "    StateGraph: The configured state graph for the workflow.\n",
    "    \"\"\"\n",
    "    graph = StateGraph(AgentGraphState)\n",
    "    # graph.add_node(\n",
    "    #     \"planner\",\n",
    "    #     lambda state: PlannerAgent(\n",
    "    #         state=state,\n",
    "    #         model_config=ollama_config,\n",
    "    #     ).invoke(\n",
    "    #         research_question=state[\"research_question\"],\n",
    "    #         feedback=lambda: get_agent_graph_state(\n",
    "    #             state=state, state_key=\"reviewer_latest\"\n",
    "    #         ),\n",
    "    #         prompt=planner_prompt_template,\n",
    "    #     ),\n",
    "    # )\n",
    "\n",
    "    graph.add_node(\n",
    "        \"start\",\n",
    "        lambda state: StartNodeAgent(\n",
    "            state=state,\n",
    "            model_config=ollama_config,\n",
    "        ).invoke(),\n",
    "    )\n",
    "\n",
    "    graph.add_node(\n",
    "        \"audio\",\n",
    "        lambda state: AudioAgent(\n",
    "            state=state,\n",
    "        ).invoke(\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    graph.add_node(\n",
    "        \"transcript\",\n",
    "        lambda state: TranscriptAgent(\n",
    "            state=state,\n",
    "            model_config=ollama_config,\n",
    "        ).invoke(\n",
    "            audio_queue=lambda: get_agent_graph_state(\n",
    "                state=state, state_key=\"audio_queue\"\n",
    "            ),\n",
    "            length_queue=lambda: get_agent_graph_state(\n",
    "                state=state, state_key=\"length_queue\"\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    graph.add_node(\n",
    "        \"end\",\n",
    "        lambda state: EndNodeAgent(\n",
    "            state=state,\n",
    "            model_config=ollama_config,\n",
    "        ).invoke(),\n",
    "    )\n",
    "\n",
    "    # Set the entry and finish points for the workflow\n",
    "    graph.set_entry_point(\"start\")\n",
    "    graph.set_finish_point(\"end\")\n",
    "\n",
    "    # Define the flow of the graph\n",
    "    graph.add_edge(\"start\", \"audio\")\n",
    "    graph.add_edge(\"start\", \"transcript\")\n",
    "\n",
    "    graph.add_conditional_edges(\n",
    "        \"transcript\", should_continue, {\"transcript\": \"transcript\", \"end\": \"end\"}\n",
    "    )\n",
    "    graph.add_edge(\"audio\", \"end\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def compile_workflow(graph: StateGraph):\n",
    "    \"\"\"\n",
    "    Compiles the state graph into an executable workflow.\n",
    "\n",
    "    This function compiles the graph, enabling the defined nodes and edges to\n",
    "    be executed in sequence as per the workflow's logic.\n",
    "\n",
    "    Parameters:\n",
    "    graph (StateGraph): The state graph defining the workflow.\n",
    "\n",
    "    Returns:\n",
    "    Any: The compiled workflow ready for execution.\n",
    "    \"\"\"\n",
    "    workflow = graph.compile()\n",
    "    return workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Workflow\n",
    "\n",
    "In this final section, we execute the workflow defined in the agent graph. We start by creating the graph using the `create_graph` function and then compiling it into an executable workflow with `compile_workflow`. The workflow is then run with specific inputs and configurations.\n",
    "\n",
    "- **`graph = create_graph()`:** This initializes the graph structure, including all nodes and edges as defined previously.\n",
    "- **`workflow = compile_workflow(graph)`:** This compiles the graph into a runnable workflow, preparing it for execution.\n",
    "- **`iterations = 10`:** This variable sets the recursion limit for the workflow, determining how many iterations the workflow should allow.\n",
    "- **`verbose = True`:** If set to `True`, the system will print detailed information about each state change during the workflow execution.\n",
    "- **`query = \"Who's the president of the USA?\"`:** The research question provided as input to the workflow, which the planner agent will process.\n",
    "- **`dict_inputs = {\"research_question\": query}`:** A dictionary containing the initial inputs to the workflow, including the research question.\n",
    "- **`limit = {\"recursion_limit\": iterations}`:** This sets the limit for the number of iterations, preventing infinite loops or excessive processing.\n",
    "\n",
    "The loop iterates over the events generated by the workflow, printing the state at each step if `verbose` is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph and compile the workflow\n",
    "graph = create_graph()\n",
    "workflow = compile_workflow(graph)\n",
    "print(\"Graph and workflow created.\")\n",
    "\n",
    "# Define workflow parameters\n",
    "iterations = 10\n",
    "verbose = True\n",
    "dict_inputs = {\n",
    "    \"audio_queue\": queue.Queue(),\n",
    "    \"length_queue\": queue.Queue(maxsize=LENGTH_IN_SEC),\n",
    "}\n",
    "limit = {\"recursion_limit\": iterations}\n",
    "\n",
    "# Execute the workflow and print state changes\n",
    "for event in workflow.stream(dict_inputs, limit):\n",
    "    if verbose:\n",
    "        print(\"\\nState Dictionary:\", event)\n",
    "    else:\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
