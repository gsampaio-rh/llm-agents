{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LLM Agents with LLama3 and LangGraph\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, you will learn how to build and deploy LLM (Large Language Model) agents using LLama3 and LangGraph. These agents will be capable of processing natural language inputs, generating comprehensive plans, and handling complex workflows. By the end of this notebook, you will understand how to:\n",
    "\n",
    "1. Set up and configure a language model using LLama3.\n",
    "2. Define and manage the state for agents in a workflow.\n",
    "3. Implement and customize agent classes for specific tasks.\n",
    "4. Construct and compile a workflow graph using LangGraph.\n",
    "5. Execute the workflow and handle outputs effectively.\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "Before diving into the implementation, it's essential to understand some basic concepts:\n",
    "\n",
    "- **LLM (Large Language Model):** A machine learning model trained on vast amounts of text data to understand and generate human-like language. LLama3 is an example of such a model.\n",
    "\n",
    "- **Agent:** A software component that interacts with the LLM to perform specific tasks, such as generating responses or processing information. In this notebook, we implement agents that can handle various aspects of a workflow.\n",
    "\n",
    "- **State:** A shared data structure that stores the context and data required by agents. The state is critical for maintaining continuity and passing information between different parts of the workflow.\n",
    "\n",
    "- **Workflow Graph:** A structured representation of the workflow, where nodes represent agents and edges define the flow of information and control. LangGraph is used to construct and manage these workflow graphs.\n",
    "\n",
    "- **Prompt Engineering:** The process of crafting prompts that guide the LLM's responses. Proper prompt engineering is crucial for ensuring the model generates relevant and accurate outputs.\n",
    "\n",
    "Understanding these concepts will provide a solid foundation as we proceed with the practical implementation of LLM agents and workflows in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "Before we dive into building our agent, we need to set up the necessary environment. This involves installing required packages and ensuring our Python environment is ready for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: termcolor in ./.conda/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: langgraph in ./.conda/lib/python3.11/site-packages (0.1.15)\n",
      "Requirement already satisfied: langchain-community in ./.conda/lib/python3.11/site-packages (0.2.10)\n",
      "Requirement already satisfied: arxiv in ./.conda/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: wikipedia in ./.conda/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: duckduckgo-search in ./.conda/lib/python3.11/site-packages (6.2.4)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2.22 in ./.conda/lib/python3.11/site-packages (from langgraph) (0.2.23)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.11/site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/lib/python3.11/site-packages (from langchain-community) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.conda/lib/python3.11/site-packages (from langchain-community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.conda/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.9 in ./.conda/lib/python3.11/site-packages (from langchain-community) (0.2.11)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./.conda/lib/python3.11/site-packages (from langchain-community) (0.1.93)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.conda/lib/python3.11/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.conda/lib/python3.11/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.conda/lib/python3.11/site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in ./.conda/lib/python3.11/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.conda/lib/python3.11/site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: click>=8.1.7 in ./.conda/lib/python3.11/site-packages (from duckduckgo-search) (8.1.7)\n",
      "Requirement already satisfied: primp>=0.5.4 in ./.conda/lib/python3.11/site-packages (from duckduckgo-search) (0.5.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: sgmllib3k in ./.conda/lib/python3.11/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.conda/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.9->langchain-community) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.conda/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.9->langchain-community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2.22->langgraph) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2.22->langgraph) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in ./.conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.conda/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.22->langgraph) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.9->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.9->langchain-community) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install termcolor for colored terminal outputs\n",
    "%pip install termcolor langgraph langchain-community arxiv wikipedia duckduckgo-search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from termcolor import colored\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Configuration\n",
    "\n",
    "In this section, we set up the configuration for the Ollama model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Ollama Model\n",
    "\n",
    "The `setup_ollama_model` function configures the model settings, including the endpoint, model name, system prompt, and other parameters. This setup is essential for initializing the model with the correct configuration, ensuring it can process queries and utilize the tools effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ollama_model(model, temperature=0, stop=None):\n",
    "    \"\"\"\n",
    "    Sets up the Ollama model configuration.\n",
    "\n",
    "    Parameters:\n",
    "    model (str): The name of the model to use.\n",
    "    temperature (float): The temperature setting for the model.\n",
    "    stop (str): The stop token for the model.\n",
    "\n",
    "    Returns:\n",
    "    dict: Configuration for the Ollama model.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model_endpoint\": \"http://localhost:11434/api/generate\",\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"stop\": stop,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example configuration\n",
    "ollama_config = setup_ollama_model(model=\"llama3:instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'duckduckgo_search'"
      ]
     },
     "execution_count": 927,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "duckSearch = DuckDuckGoSearchRun()\n",
    "duckSearch.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 928,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 929,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "\n",
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/Users/gsampaio/redhat/ai/llm-agents/.conda/lib/python3.11/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " DuckDuckGoSearchRun()]"
      ]
     },
     "execution_count": 930,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [wiki, arxiv, duckSearch]\n",
    "\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia - A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query., args: {{'query': {{'title': 'Query', 'description': 'query to look up on wikipedia', 'type': 'string'}}}}\n",
      "arxiv - A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query., args: {{'query': {{'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}}}\n",
      "duckduckgo_search - A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query., args: {{'query': {{'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools.render import render_text_description_and_args\n",
    "\n",
    "tools_description = (\n",
    "    render_text_description_and_args(tools).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    ")\n",
    "\n",
    "print(tools_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defining the Agent Graph State\n",
    "\n",
    "In this step, we define the structure of the state that our agents will use to store and communicate information. This state acts as a shared memory that different components of the system can access and modify. We use the `TypedDict` from the `typing` module to define the expected structure and types of data within the state. This helps ensure consistency and correctness when accessing or updating the state, making it easier to manage complex workflows and data dependencies.\n",
    "\n",
    "The `AgentGraphState` class includes fields for the research question, responses from the planner agent, and any final outputs or end states. The `get_agent_graph`_state function is used to retrieve specific parts of the state based on a key, facilitating modular and reusable access to the state data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# Define the state object for the agent graph\n",
    "class AgentGraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    This class defines the structure of the agent graph state.\n",
    "\n",
    "    Attributes:\n",
    "    research_question (str): The main research question the agent is working on.\n",
    "    planner_response (list): A list to store responses from the planner agent.\n",
    "    end_chain (list): A list to store the final outputs or end states.\n",
    "    \"\"\"\n",
    "\n",
    "    research_question: str\n",
    "    planner_response: Annotated[list, add_messages]\n",
    "    router_response: Annotated[list, add_messages]\n",
    "    researcher_response: Annotated[list, add_messages]\n",
    "    reviewer_response: Annotated[list, add_messages]\n",
    "    end_chain: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# Function to retrieve specific parts of the agent state\n",
    "def get_agent_graph_state(state: AgentGraphState, state_key: str):\n",
    "    \"\"\"\n",
    "    Retrieves specific parts of the agent state based on the provided key.\n",
    "\n",
    "    Parameters:\n",
    "    state (AgentGraphState): The current state of the agent.\n",
    "    state_key (str): The key indicating which part of the state to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    list or None: The requested state data or None if the key is not recognized.\n",
    "    \"\"\"\n",
    "\n",
    "    if state_key == \"router_all\":\n",
    "        return state[\"router_response\"]\n",
    "    elif state_key == \"router_latest\":\n",
    "        return state[\"router_response\"][-1] if state[\"router_response\"] else []\n",
    "\n",
    "    if state_key == \"planner_all\":\n",
    "        return state[\"planner_response\"]\n",
    "    elif state_key == \"planner_latest\":\n",
    "        return state[\"planner_response\"][-1] if state[\"planner_response\"] else []\n",
    "\n",
    "    if state_key == \"researcher_all\":\n",
    "        return state[\"researcher_response\"]\n",
    "    elif state_key == \"researcer_latest\":\n",
    "        return state[\"researcher_response\"][-1] if state[\"researcher_response\"] else []\n",
    "\n",
    "    if state_key == \"reviewer_all\":\n",
    "        return state[\"reviewer_response\"]\n",
    "    elif state_key == \"reviewer_latest\":\n",
    "        return state[\"reviewer_response\"][-1] if state[\"reviewer_response\"] else []\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Initial state setup\n",
    "state = {\n",
    "    \"research_question\": \"\",\n",
    "    \"router_response\": [],\n",
    "    \"planner_response\": [],\n",
    "    \"researcher_response\": [],\n",
    "    \"reviewer_response\": [],\n",
    "    \"end_chain\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agent Class Definition\n",
    "\n",
    "In this section, we define the `Agent` class, which serves as a base class for different types of agents in our system. An agent is a component that interacts with the language model to perform specific tasks, such as generating responses or processing information. The `Agent` class manages the configuration and state associated with the language model, allowing for easy setup and reuse of model configurations across different agents.\n",
    "\n",
    "The class includes methods for initializing the agent with a specific model configuration and updating the agent's state. The state encapsulates the context or memory of the agent, enabling it to maintain continuity across interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state: AgentGraphState, model_config: dict):\n",
    "        \"\"\"\n",
    "        Initializes the agent with a state and model configuration.\n",
    "\n",
    "        Parameters:\n",
    "        state (AgentGraphState): The initial state of the agent, containing necessary context and data.\n",
    "        model_config (dict): Configuration settings for the model, including endpoint, model name, temperature, etc.\n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        self.model_endpoint = model_config.get(\"model_endpoint\")\n",
    "        self.model_name = model_config.get(\"model\")\n",
    "        self.temperature = model_config.get(\n",
    "            \"temperature\", 0\n",
    "        )  # Default temperature is 0\n",
    "        self.headers = model_config.get(\"headers\", {\"Content-Type\": \"application/json\"})\n",
    "        self.stop = model_config.get(\"stop\")\n",
    "\n",
    "    def update_state(self, key: str, value: any):\n",
    "        \"\"\"\n",
    "        Updates the agent's state with a new key-value pair.\n",
    "\n",
    "        Parameters:\n",
    "        key (str): The key in the state dictionary to update.\n",
    "        value (any): The new value to associate with the specified key.\n",
    "        \"\"\"\n",
    "        if key in self.state:\n",
    "            self.state[key] = value\n",
    "        else:\n",
    "            print(f\"Warning: Attempting to update a non-existing state key '{key}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Utility Functions\n",
    "\n",
    "Utility functions are auxiliary functions that assist with various common tasks within the notebook. They help keep the codebase clean and modular by encapsulating frequently used logic in separate functions. In this case, we have two utility functions: `check_for_content` and `get_current_utc_datetime`.\n",
    "\n",
    "- `check_for_content`: This function checks if a variable has a content attribute and returns its value if it exists. This is useful for handling different data types that may or may not have a content attribute.\n",
    "- `get_current_utc_datetime`: This function returns the current date and time in UTC format. This can be useful for timestamping events or logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "# Check if an attribute of the state dict has content\n",
    "def check_for_content(var):\n",
    "    \"\"\"\n",
    "    Checks if the provided variable has a 'content' attribute and returns it.\n",
    "\n",
    "    Parameters:\n",
    "    var (Any): The variable to check.\n",
    "\n",
    "    Returns:\n",
    "    Any: The 'content' attribute if it exists, otherwise the original variable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return var.content\n",
    "    except AttributeError:\n",
    "        return var\n",
    "\n",
    "\n",
    "# Get the current date and time in UTC\n",
    "def get_current_utc_datetime():\n",
    "    \"\"\"\n",
    "    Returns the current date and time in UTC.\n",
    "\n",
    "    Returns:\n",
    "    str: The current date and time in UTC, formatted as 'YYYY-MM-DD HH:MM:SS UTC'.\n",
    "    \"\"\"\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    return now_utc.strftime(\"%Y-%m-%d %H:%M:%S UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Router Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt_template = \"\"\"\n",
    "You are a router. Your task is to route the conversation to the next agent based on the feedback provided by the reviewer.\n",
    "You must choose one of the following agents: planner, selector, reporter, or final_report.\n",
    "\n",
    "Here is the feedback provided by the reviewer:\n",
    "Feedback: {feedback}\n",
    "\n",
    "Current date and time:\n",
    "{datetime}\n",
    "\n",
    "### Criteria for Choosing the Next Agent:\n",
    "- **planner**: Responsible for planning all necessary steps and determining which agents need to be called to complete the research.\n",
    "- **researcher**: Equipped with tools to gather detailed and accurate information from various sources based on the planner's guidance.\n",
    "- **reviewer**: Reviews the information gathered by the researcher, ensuring the results are clear, comprehensive, and accurate.\n",
    "\n",
    "you must provide your response in the following json format:\n",
    "    \n",
    "        \"next_agent\": \"one of the following: planner/researcher/reviewer\"\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "router_guided_json = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"next_agent\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"one of the following: planner/researcher/reviewer\",\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"next_agent\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "class RouterAgent(Agent):\n",
    "\n",
    "    def invoke(\n",
    "        self,\n",
    "        research_question: str,\n",
    "        prompt: str = router_prompt_template,\n",
    "        feedback: any = None,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Generates a response from the model based on the provided prompt.\n",
    "\n",
    "        Parameters:\n",
    "        research_question (str): The research question the planner agent needs to address.\n",
    "        prompt (str): The template used to generate the system prompt.\n",
    "        feedback (callable or str): Feedback received to adjust the planning process.\n",
    "\n",
    "        Returns:\n",
    "        dict: The updated state of the agent after processing the response.\n",
    "        \"\"\"\n",
    "        feedback_value = feedback() if callable(feedback) else feedback\n",
    "        feedback_value = check_for_content(feedback_value)\n",
    "\n",
    "        router_prompt = prompt.format(\n",
    "            feedback=feedback_value, datetime=get_current_utc_datetime()\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": router_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Research question: {research_question}\"},\n",
    "        ]\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"format\": \"json\",\n",
    "            \"prompt\": messages[1][\"content\"],\n",
    "            \"system\": messages[0][\"content\"],\n",
    "            \"stream\": False,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.model_endpoint, headers=self.headers, data=json.dumps(payload)\n",
    "            )\n",
    "            response_json = response.json()\n",
    "            response_content = json.loads(response_json.get(\"response\", \"{}\"))\n",
    "            response_formatted = HumanMessage(content=json.dumps(response_content))\n",
    "\n",
    "            print(colored(f\"Router ðŸ§­: {response_formatted}\", \"blue\"))\n",
    "            self.update_state(\"router_response\", response_formatted)\n",
    "            return self.state\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error in invoking model! {str(e)}\")\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PlannerAgent\n",
    "\n",
    "In this section, we define the `planner_prompt_template` and the `PlannerAgent` class. The `planner_prompt_template` is a string template that sets the context and expectations for the planner agent's response. It guides the agent in generating a comprehensive plan to help answer a given research question. The planner's responsibilities include identifying the most relevant search term, outlining an overall strategy, and providing additional information or filters for a thorough search.\n",
    "\n",
    "The `PlannerAgent` class extends the base `Agent` class and implements specific functionality for the planning task. It uses the provided template to format its requests to the language model and processes the model's responses. The class includes methods like `think`, which are used to generate and retrieve the agent's outputs based on the research question and any feedback received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for guiding the planner agent's response\n",
    "planner_prompt_template = \"\"\"\n",
    "You are a planner. Your responsibility is to create a comprehensive plan to help your team answer a research question.\n",
    "Questions may vary from simple to complex, multi-step queries. Your plan should provide appropriate guidance for your \n",
    "team, assigning tasks to the appropriate agents based on their responsibilities.\n",
    "\n",
    "### Agent Descriptions:\n",
    "- **Researcher**: Equipped with tools to gather detailed and accurate information from various sources based on the planner's guidance.\n",
    "- **Reviewer**: Reviews the information gathered by the researcher, ensuring the results are clear, comprehensive, and accurate.\n",
    "\n",
    "Your plan must detail each step, specifying the task, the responsible agent, and the expected result. If you receive \n",
    "feedback, adjust your plan accordingly. Here is the feedback received:\n",
    "Feedback: {feedback}\n",
    "\n",
    "Current date and time:\n",
    "{datetime}\n",
    "\n",
    "Your response must take the following JSON format:\n",
    "\n",
    "    \"steps\"\n",
    "        \n",
    "            \"step_number\": 1,\n",
    "            \"task\": \"Description of the task\",\n",
    "            \"agent\": \"Assigned agent (researcher/reviewer)\",\n",
    "            \"expected_result\": \"Expected outcome of this step\"\n",
    "        \n",
    "        ...\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# JSON schema for the planner's response\n",
    "planner_guided_json = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"steps\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"step_number\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The step number in the sequence\",\n",
    "                    },\n",
    "                    \"task\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the task\",\n",
    "                    },\n",
    "                    \"agent\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Assigned agent (researcher/reviewer)\",\n",
    "                    },\n",
    "                    \"expected_result\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Expected outcome of this step\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"step_number\", \"task\", \"agent\", \"expected_result\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"steps\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlannerAgent(Agent):\n",
    "\n",
    "    def invoke(\n",
    "        self,\n",
    "        research_question: str,\n",
    "        prompt: str = planner_prompt_template,\n",
    "        feedback: any = None,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Generates a response from the model based on the provided prompt.\n",
    "\n",
    "        Parameters:\n",
    "        research_question (str): The research question the planner agent needs to address.\n",
    "        prompt (str): The template used to generate the system prompt.\n",
    "        feedback (callable or str): Feedback received to adjust the planning process.\n",
    "\n",
    "        Returns:\n",
    "        dict: The updated state of the agent after processing the response.\n",
    "        \"\"\"\n",
    "        feedback_value = feedback() if callable(feedback) else feedback\n",
    "        feedback_value = check_for_content(feedback_value)\n",
    "\n",
    "        planner_prompt = prompt.format(\n",
    "            feedback=feedback_value, datetime=get_current_utc_datetime()\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": planner_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Research question: {research_question}\"},\n",
    "        ]\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"format\": \"json\",\n",
    "            \"prompt\": messages[1][\"content\"],\n",
    "            \"system\": messages[0][\"content\"],\n",
    "            \"stream\": False,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.model_endpoint, headers=self.headers, data=json.dumps(payload)\n",
    "            )\n",
    "            response_json = response.json()\n",
    "            response_content = json.loads(response_json.get(\"response\", \"{}\"))\n",
    "            response_formatted = HumanMessage(content=json.dumps(response_content))\n",
    "\n",
    "            self.update_state(\"planner_response\", response_formatted)\n",
    "            print(colored(f\"Planner ðŸ‘©ðŸ¿â€ðŸ’»: {response_formatted}\", \"cyan\"))\n",
    "            return self.state\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error in invoking model! {str(e)}\")\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Researcher Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for guiding the researcher agent's response\n",
    "researcher_prompt_template = \"\"\"\n",
    "You are a researcher. Your task is to gather detailed and accurate information based on the assigned task. Use the provided \n",
    "search terms and guidelines to select the appropriate tool for gathering information. Specify the selected tool and provide \n",
    "the exact input (arguments and parameters) required for the tool.\n",
    "\n",
    "If you receive feedback, adjust your plan accordingly. Here is the feedback received:\n",
    "Feedback: {feedback}\n",
    "\n",
    "### Current date and time:\n",
    "{datetime}\n",
    "\n",
    "**Important**:\n",
    "1. The input should be provided as key-value pairs, where the keys are the argument names (e.g., `query`) and the values are the actual inputs (e.g., `\"Current President of the United States\"`).\n",
    "2. Do not include additional metadata such as `title`, `description`, or `type` in the `tool_input`.\n",
    "3. The `tool_input` should directly map the required arguments to their values as per the tool's specifications.\n",
    "\n",
    "### Tools Description:\n",
    "{tools_description}\n",
    "\n",
    "### Input from Planner:\n",
    "{planner_instruction}\n",
    "\n",
    "Your response must take the following JSON format:\n",
    "\n",
    "    \"selected_tool\": \"The tool chosen for the research\",\n",
    "    \"tool_input\":\n",
    "        \"argument_name\": \"argument_value\"\n",
    "        ...\n",
    "        \n",
    "**Correct Example**:\n",
    "- Input: \"query\": \"Current President of the United States\"\n",
    "\n",
    "**Incorrect Example**:\n",
    "- Input: \"query\": \n",
    "    \"title\": \"Current President of the United States\",\n",
    "    \"description\": \"current president of the usa\", \n",
    "    \"type\": \"string\"\n",
    "\n",
    "        \n",
    "Remember:\n",
    "- Do not include additional metadata such as `title`, `description`, or `type` in the `tool_input`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# JSON schema for the researcher's response\n",
    "researcher_guided_json = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"selected_tool\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The tool chosen for the research\",\n",
    "        },\n",
    "        \"tool_input\": {\n",
    "            \"type\": \"object\",\n",
    "            \"additionalProperties\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Arguments to be inputted into the tool\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"selected_tool\", \"tool_input\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearcherAgent(Agent):\n",
    "\n",
    "    def invoke(\n",
    "        self,\n",
    "        research_question: str,\n",
    "        instruction: str,\n",
    "        prompt: str = researcher_prompt_template,\n",
    "        feedback: any = None,\n",
    "        tools_description: str = tools_description,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Generates a response from the model based on the provided prompt.\n",
    "\n",
    "        Parameters:\n",
    "        research_question (str): The research question the planner agent needs to address.\n",
    "        prompt (str): The template used to generate the system prompt.\n",
    "        feedback (callable or str): Feedback received to adjust the planning process.\n",
    "\n",
    "        Returns:\n",
    "        dict: The updated state of the agent after processing the response.\n",
    "        \"\"\"\n",
    "        feedback_value = feedback() if callable(feedback) else feedback\n",
    "        feedback_value = check_for_content(feedback_value)\n",
    "\n",
    "        researcher_prompt = prompt.format(\n",
    "            feedback=feedback_value,\n",
    "            planner_instruction=instruction,\n",
    "            tools_description=tools_description,\n",
    "            datetime=get_current_utc_datetime(),\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": researcher_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Research question: {research_question}\"},\n",
    "        ]\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"format\": \"json\",\n",
    "            \"prompt\": messages[1][\"content\"],\n",
    "            \"system\": messages[0][\"content\"],\n",
    "            \"stream\": False,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.model_endpoint, headers=self.headers, data=json.dumps(payload)\n",
    "            )\n",
    "            response_json = response.json()\n",
    "            response_content = json.loads(response_json.get(\"response\", \"{}\"))\n",
    "            response_formatted = HumanMessage(content=json.dumps(response_content))\n",
    "\n",
    "            self.update_state(\"researcher_response\", response_formatted)\n",
    "            print(colored(f\"Researcher ðŸ•µï¸â€â™‚ï¸: {response_formatted}\", \"yellow\"))\n",
    "            return self.state\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error in invoking model! {str(e)}\")\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tools Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# This a helper class we have that is useful for running tools\n",
    "# It takes in an agent action and calls that tool and returns the result\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "# Define the function to execute tools\n",
    "def execute_tools(data):\n",
    "    \"\"\"\n",
    "    Executes the tool selected by the ToolsAgent.\n",
    "\n",
    "    Parameters:\n",
    "    data (dict): The data containing the agent's state, including the selected tool.\n",
    "\n",
    "    Returns:\n",
    "    dict: Updated state with the tool execution results.\n",
    "    \"\"\"\n",
    "    # Get the most recent agent_outcome - this is the key added in the ToolsAgent\n",
    "    researcher_response = data[\"researcher_response\"][-1]\n",
    "    if isinstance(researcher_response, HumanMessage):\n",
    "        researcher_response_json = json.loads(researcher_response.content)\n",
    "        selected_tool = researcher_response_json.get(\"selected_tool\")\n",
    "        tool_input = researcher_response_json.get(\"tool_input\")\n",
    "        if selected_tool:\n",
    "            # Execute the selected tool and store the output\n",
    "            print(\n",
    "                colored(\n",
    "                    f\"Selected Tool ðŸª›: {selected_tool} Input:{tool_input}\",\n",
    "                    \"light_magenta\",\n",
    "                )\n",
    "            )\n",
    "            try:\n",
    "                invocation = ToolInvocation(tool=selected_tool, tool_input=tool_input)\n",
    "                output = tool_executor.invoke(invocation)\n",
    "                print(colored(f\"Result:\\n{output}\", \"magenta\"))\n",
    "\n",
    "                # Update the state with the tool's response\n",
    "                researcher_tools_response = SystemMessage(content=output)\n",
    "                return {\"researcher_response\": researcher_tools_response}\n",
    "            except Exception as e:\n",
    "                output = f\"Error executing tool {selected_tool}: {str(e)}\"\n",
    "                print(colored(output, \"red\"))\n",
    "                researcher_tools_response = SystemMessage(content=output)\n",
    "                return {\"researcher_response\": researcher_tools_response}\n",
    "        else:\n",
    "            print(\"Error: No selected_tool specified in the HumanMessage.\")\n",
    "            return {\"researcher_response\": researcher_response}\n",
    "    else:\n",
    "        print(\"Error: researcher_response is not of type HumanMessage.\")\n",
    "        return {\"researcher_response\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for guiding the reviewer agent's response\n",
    "reviewer_prompt_template = \"\"\"\n",
    "You are a reviewer. Your task is to evaluate the information provided by the researcher. You need to check the accuracy, relevance, and completeness of the data. Verify whether the gathered information answers the research question accurately and meets the expected results outlined by the planner.\n",
    "\n",
    "### Current date and time:\n",
    "{datetime}\n",
    "\n",
    "### Input from Researcher:\n",
    "{researcher_results}\n",
    "\n",
    "You should consider the previous feedback you have given when providing new feedback.\n",
    "### Feedback: \n",
    "{feedback}\n",
    "\n",
    "Your response must take the following JSON format:\n",
    "\n",
    "    \"verification_status\": \"True if the information is accurate and complete, False otherwise\",\n",
    "    \"feedback\": \"Detailed feedback on the accuracy, relevance, and completeness of the information. Include any necessary corrections or additional information needed.\"\n",
    "\"\"\"\n",
    "\n",
    "# JSON schema for the reviewer's response\n",
    "reviewer_guided_json = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"verification_status\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"True if the information is accurate and complete, False otherwise\",\n",
    "        },\n",
    "        \"feedback\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Detailed feedback on the accuracy, relevance, and completeness of the information\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"verification_status\", \"feedback\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewerAgent(Agent):\n",
    "\n",
    "    def invoke(\n",
    "        self,\n",
    "        research_question: str,\n",
    "        research: str,\n",
    "        prompt: str = reviewer_prompt_template,\n",
    "        feedback: any = None,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Generates a response from the model based on the provided prompt.\n",
    "\n",
    "        Parameters:\n",
    "        research_question (str): The research question the planner agent needs to address.\n",
    "        prompt (str): The template used to generate the system prompt.\n",
    "        feedback (callable or str): Feedback received to adjust the planning process.\n",
    "\n",
    "        Returns:\n",
    "        dict: The updated state of the agent after processing the response.\n",
    "        \"\"\"\n",
    "        feedback_value = feedback() if callable(feedback) else feedback\n",
    "        feedback_value = check_for_content(feedback_value)\n",
    "\n",
    "        reviewer_prompt = prompt.format(\n",
    "            feedback=feedback_value,\n",
    "            researcher_results=research,\n",
    "            tools_description=tools_description,\n",
    "            datetime=get_current_utc_datetime(),\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": reviewer_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Research question: {research_question}\"},\n",
    "        ]\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"format\": \"json\",\n",
    "            \"prompt\": messages[1][\"content\"],\n",
    "            \"system\": messages[0][\"content\"],\n",
    "            \"stream\": False,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.model_endpoint, headers=self.headers, data=json.dumps(payload)\n",
    "            )\n",
    "            response_json = response.json()\n",
    "            response_content = json.loads(response_json.get(\"response\", \"{}\"))\n",
    "            response_formatted = HumanMessage(content=json.dumps(response_content))\n",
    "\n",
    "            self.update_state(\"reviewer_response\", response_formatted)\n",
    "            print(colored(f\"Reviewer ðŸ•µï¸â€â™‚ï¸: {response_formatted}\", \"green\"))\n",
    "            return self.state\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error in invoking model! {str(e)}\")\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. End Node\n",
    "\n",
    "The `EndNodeAgent` class is a specialized agent that marks the conclusion of the workflow in the agent graph. It extends the base `Agent` class and is primarily responsible for updating the state to indicate the end of the process. This agent is useful for workflows that require a clear termination point, ensuring that the system knows when all processing is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndNodeAgent(Agent):\n",
    "    def invoke(self) -> AgentGraphState:\n",
    "        \"\"\"\n",
    "        Marks the end of the workflow by updating the state.\n",
    "\n",
    "        This method updates the 'end_chain' key in the state to signify that\n",
    "        the workflow has reached its conclusion. It can be used to perform any\n",
    "        finalization tasks or simply to denote that the agent has completed its role.\n",
    "\n",
    "        Returns:\n",
    "        AgentGraphState: The updated state of the agent.\n",
    "        \"\"\"\n",
    "        self.update_state(\"end_chain\", \"end_chain\")\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(data):\n",
    "    \"\"\"\n",
    "    Determines the next step in the workflow based on the agent's output.\n",
    "\n",
    "    Parameters:\n",
    "    data (dict): The data containing the agent's state.\n",
    "\n",
    "    Returns:\n",
    "    str: The next node to execute ('continue' for tool execution, 'end' to finish).\n",
    "    \"\"\"\n",
    "    # Check if the tools_response contains a final answer or indication to stop\n",
    "\n",
    "    reviewer_response = data[\"reviewer_response\"][-1]\n",
    "    reviewer_response_json = json.loads(reviewer_response.content)\n",
    "    \n",
    "    verification_status = reviewer_response_json.get(\"verification_status\", False)\n",
    "    feedback = reviewer_response_json.get(\"feedback\", \"\")\n",
    "\n",
    "    # Determine the next step based on the verification status\n",
    "    if verification_status:\n",
    "        print(colored(f\"Reviewer ðŸ•µï¸â€â™‚ï¸: {feedback}\", \"green\"))\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(colored(f\"Reviewer ðŸ•µï¸â€â™‚ï¸: {feedback}\", \"red\"))\n",
    "        return \"replan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Creating and Compiling the Agent Graph\n",
    "\n",
    "In this section, we define the structure and flow of the agent-based system using the `StateGraph` class from the `langgraph` library. The graph consists of nodes, each representing a specific agent, and edges, which define the flow or sequence of operations. This setup enables the modeling of complex workflows where different agents can interact and pass information.\n",
    "\n",
    "- **`create_graph`:** This function initializes the `StateGraph` with a specific state structure (`AgentGraphState`). It then adds nodes for the `PlannerAgent` and `EndNodeAgent`, specifying the operations these agents should perform. The function sets the \"planner\" node as the entry point and the \"end\" node as the finish point, with an edge connecting them to define the workflow sequence.\n",
    "- **`compile_workflow`:** This function compiles the defined graph into a workflow that can be executed. The compiled workflow manages the execution of the nodes in the defined order, handling the flow of data and control through the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def create_graph() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Creates and configures the state graph for the agent workflow.\n",
    "\n",
    "    This function initializes the graph, adds the necessary nodes (agents), and\n",
    "    sets up the edges defining the flow of the workflow.\n",
    "\n",
    "    Returns:\n",
    "    StateGraph: The configured state graph for the workflow.\n",
    "    \"\"\"\n",
    "    graph = StateGraph(AgentGraphState)\n",
    "\n",
    "    graph.add_node(\n",
    "        \"router\",\n",
    "        lambda state: RouterAgent(\n",
    "            state=state,\n",
    "            model_config=ollama_config,\n",
    "        ).invoke(\n",
    "            research_question=state[\"research_question\"],\n",
    "            feedback=lambda: get_agent_graph_state(\n",
    "                state=state, state_key=\"reviewer_latest\"\n",
    "            ),\n",
    "            prompt=router_prompt_template,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    graph.add_node(\n",
    "        \"planner\",\n",
    "        lambda state: PlannerAgent(\n",
    "            state=state,\n",
    "            model_config=ollama_config,\n",
    "        ).invoke(\n",
    "            research_question=state[\"research_question\"],\n",
    "            feedback=lambda: get_agent_graph_state(\n",
    "                state=state, state_key=\"reviewer_latest\"\n",
    "            ),\n",
    "            prompt=planner_prompt_template,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    graph.add_node(\n",
    "        \"researcher\",\n",
    "        lambda state: ResearcherAgent(\n",
    "            state=state,\n",
    "            model_config=ollama_config,\n",
    "        ).invoke(\n",
    "            research_question=state[\"research_question\"],\n",
    "            feedback=lambda: get_agent_graph_state(\n",
    "                state=state, state_key=\"reviewer_latest\"\n",
    "            ),\n",
    "            instruction=lambda: get_agent_graph_state(\n",
    "                state=state, state_key=\"planner_latest\"\n",
    "            ),\n",
    "            prompt=researcher_prompt_template,\n",
    "            tools_description=tools_description,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    graph.add_node(\"execute_tools\", execute_tools)\n",
    "\n",
    "    graph.add_node(\n",
    "        \"reviewer\",\n",
    "        lambda state: ReviewerAgent(\n",
    "            state=state,\n",
    "            model_config=ollama_config,\n",
    "        ).invoke(\n",
    "            research_question=state[\"research_question\"],\n",
    "            feedback=lambda: get_agent_graph_state(\n",
    "                state=state, state_key=\"reviewer_latest\"\n",
    "            ),\n",
    "            research=lambda: get_agent_graph_state(\n",
    "                state=state, state_key=\"research_latest\"\n",
    "            ),\n",
    "            prompt=reviewer_prompt_template,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    graph.add_node(\n",
    "        \"end\",\n",
    "        lambda state: EndNodeAgent(\n",
    "            state=state,\n",
    "            model_config=ollama_config,\n",
    "        ).invoke(),\n",
    "    )\n",
    "\n",
    "    # Set the entry and finish points for the workflow\n",
    "    graph.set_entry_point(\"router\")\n",
    "    graph.set_finish_point(\"end\")\n",
    "\n",
    "    # Define the flow of the graph\n",
    "    graph.add_edge(\"router\", \"planner\")\n",
    "    graph.add_edge(\"planner\", \"researcher\")\n",
    "    graph.add_edge(\"researcher\", \"execute_tools\")\n",
    "    graph.add_edge(\"execute_tools\", \"reviewer\")\n",
    "    graph.add_conditional_edges(\n",
    "        \"reviewer\", should_continue, {\"replan\": \"planner\", \"end\": \"end\"}\n",
    "    )\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def compile_workflow(graph: StateGraph):\n",
    "    \"\"\"\n",
    "    Compiles the state graph into an executable workflow.\n",
    "\n",
    "    This function compiles the graph, enabling the defined nodes and edges to\n",
    "    be executed in sequence as per the workflow's logic.\n",
    "\n",
    "    Parameters:\n",
    "    graph (StateGraph): The state graph defining the workflow.\n",
    "\n",
    "    Returns:\n",
    "    Any: The compiled workflow ready for execution.\n",
    "    \"\"\"\n",
    "    workflow = graph.compile()\n",
    "    return workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Running the Workflow\n",
    "\n",
    "In this final section, we execute the workflow defined in the agent graph. We start by creating the graph using the `create_graph` function and then compiling it into an executable workflow with `compile_workflow`. The workflow is then run with specific inputs and configurations.\n",
    "\n",
    "- **`graph = create_graph()`:** This initializes the graph structure, including all nodes and edges as defined previously.\n",
    "- **`workflow = compile_workflow(graph)`:** This compiles the graph into a runnable workflow, preparing it for execution.\n",
    "- **`iterations = 10`:** This variable sets the recursion limit for the workflow, determining how many iterations the workflow should allow.\n",
    "- **`verbose = True`:** If set to `True`, the system will print detailed information about each state change during the workflow execution.\n",
    "- **`query = \"Who's the president of the USA?\"`:** The research question provided as input to the workflow, which the planner agent will process.\n",
    "- **`dict_inputs = {\"research_question\": query}`:** A dictionary containing the initial inputs to the workflow, including the research question.\n",
    "- **`limit = {\"recursion_limit\": iterations}`:** This sets the limit for the number of iterations, preventing infinite loops or excessive processing.\n",
    "\n",
    "The loop iterates over the events generated by the workflow, printing the state at each step if `verbose` is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph and workflow created.\n",
      "\u001b[34mRouter ðŸ§­: content='{\"next_agent\": \"researcher\"}'\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[36mPlanner ðŸ‘©ðŸ¿â€ðŸ’»: content='{\"steps\": [{\"step_number\": 1, \"task\": \"Gather information about the current president of the USA\", \"agent\": \"Researcher\", \"expected_result\": \"A list of recent US presidents in reverse chronological order\"}, {\"step_number\": 2, \"task\": \"Verify and identify the current president from the gathered information\", \"agent\": \"Researcher\", \"expected_result\": \"The name of the current president of the USA\"}]}'\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[33mResearcher ðŸ•µï¸â€â™‚ï¸: content='{\"selected_tool\": \"wikipedia\", \"tool_input\": {\"query\": \"Current President of the United States\"}}'\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95mSelected Tool ðŸª›: wikipedia Input:{'query': 'Current President of the United States'}\u001b[0m\n",
      "\u001b[35mResult:\n",
      "Page: President of the United States\n",
      "Summary: The president of the United States (POTUS) is the head of state and head of government of the United States of America. The president directs the executiv\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32mReviewer ðŸ•µï¸â€â™‚ï¸: content='{\"verification_status\": \"True\", \"feedback\": \"The provided lambda function does not contain relevant information about the current president of the USA. However, based on publicly available data and information, I can confirm that as of my knowledge cutoff in 2024, Joe Biden is indeed the current President of the United States. The accuracy and relevance of this information are confirmed.\"}'\u001b[0m\n",
      "\u001b[32mReviewer ðŸ•µï¸â€â™‚ï¸: The provided lambda function does not contain relevant information about the current president of the USA. However, based on publicly available data and information, I can confirm that as of my knowledge cutoff in 2024, Joe Biden is indeed the current President of the United States. The accuracy and relevance of this information are confirmed.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the graph and compile the workflow\n",
    "graph = create_graph()\n",
    "workflow = compile_workflow(graph)\n",
    "print(\"Graph and workflow created.\")\n",
    "\n",
    "# Define workflow parameters\n",
    "iterations = 10\n",
    "verbose = False\n",
    "query = \"Who's the current president of the USA?\"\n",
    "dict_inputs = {\"research_question\": query}\n",
    "limit = {\"recursion_limit\": iterations}\n",
    "\n",
    "# Execute the workflow and print state changes\n",
    "for event in workflow.stream(dict_inputs, limit):\n",
    "    if verbose:\n",
    "        print(\"\\nState Dictionary:\", event)\n",
    "    else:\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
